{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13022191,"sourceType":"datasetVersion","datasetId":8244908},{"sourceId":13030079,"sourceType":"datasetVersion","datasetId":8250515}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchvision timm transformers scikit-learn tqdm numpy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T20:50:11.873457Z","iopub.execute_input":"2025-09-10T20:50:11.873645Z","iopub.status.idle":"2025-09-10T20:51:24.665896Z","shell.execute_reply.started":"2025-09-10T20:50:11.873628Z","shell.execute_reply":"2025-09-10T20:51:24.665199Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### original code ","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, WeightedRandomSampler, ConcatDataset, Subset\nfrom torchvision import datasets, transforms, models\nfrom collections import Counter\nfrom PIL import Image\nimport warnings\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report\nimport zipfile\nwarnings.filterwarnings(\"ignore\")\n\n# Set device to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\nelse:\n    print(\"CUDA not available. Training on CPU.\")\n\n# Define paths for Kaggle environment\nKAGGLE_INPUT_PATH = \"/kaggle/input/plant-disease-detection\"\nKAGGLE_WORKING_PATH = \"/kaggle/input/plant-disease-detection/disease-dataset - Copy\"\nMODEL_SAVE_PATH = \"/kaggle/working/AI_Farmer_Models\"\nos.makedirs(MODEL_SAVE_PATH, exist_ok=True)\nLOCAL_DATASET_PATH = KAGGLE_WORKING_PATH\n\nclass ImprovedCNNViTHybrid(nn.Module):\n    def __init__(self, num_classes, pretrained=True):  # Fixed: __init__ instead of _init_\n        super(ImprovedCNNViTHybrid, self).__init__()  # Fixed: __init__ instead of _init_\n        self.backbone = models.resnet50(pretrained=pretrained)\n        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n        \n        self.feature_dim = 2048\n        self.patch_size = 7\n        self.num_patches = 49\n        self.embedding_dim = 768\n        \n        self.feature_projection = nn.Sequential(\n            nn.AdaptiveAvgPool2d((7, 7)),\n            nn.Conv2d(self.feature_dim, self.embedding_dim, kernel_size=1),\n            nn.BatchNorm2d(self.embedding_dim),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embedding_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, self.embedding_dim))\n        self.dropout = nn.Dropout(0.3)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.embedding_dim, nhead=8, dim_feedforward=2048, dropout=0.3, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=3)\n        \n        self.classifier = nn.Sequential(\n            nn.LayerNorm(self.embedding_dim),\n            nn.Dropout(0.3),\n            nn.Linear(self.embedding_dim, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(512, num_classes)\n        )\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        \n    def forward(self, x):\n        B = x.shape[0]\n        features = self.backbone(x)\n        features = self.feature_projection(features)\n        features = features.flatten(2).transpose(1, 2)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        features = torch.cat([cls_tokens, features], dim=1)\n        features = features + self.pos_embed\n        features = self.dropout(features)\n        encoded = self.transformer(features)\n        cls_output = encoded[:, 0]\n        return self.classifier(cls_output)\n\ndef get_transforms():\n    train_transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n        transforms.RandomHorizontalFlip(p=0.3),\n        transforms.RandomRotation(degrees=10),\n        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    return train_transform, val_transform\n\ndef filter_valid_images(dataset, split_name):\n    valid_indices = []\n    for i in tqdm(range(len(dataset)), desc=f\"Filtering valid images for {split_name}\"):\n        try:\n            _ = dataset[i]\n            valid_indices.append(i)\n        except Exception as e:\n            print(f\"Skipping invalid image at index {i}: {e}\")\n    return Subset(dataset, valid_indices)\n\ndef load_datasets_improved(crop_name, base_path=LOCAL_DATASET_PATH):\n    train_dir = os.path.join(base_path, 'Train', crop_name)\n    val_dir = os.path.join(base_path, 'Validation', crop_name)\n    test_dir = os.path.join(base_path, 'Test', crop_name)\n\n    if not os.path.exists(train_dir):\n        raise ValueError(f\"Train directory for {crop_name} not found in {train_dir}\")\n\n    train_transform, val_transform = get_transforms()\n    \n    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n    val_dataset = datasets.ImageFolder(val_dir, transform=val_transform) if os.path.exists(val_dir) else None\n    test_dataset = datasets.ImageFolder(test_dir, transform=val_transform) if os.path.exists(test_dir) else None\n\n    if val_dataset is None or test_dataset is None:\n        raise ValueError(f\"Validation or Test directory for {crop_name} not found\")\n\n    train_dataset = filter_valid_images(train_dataset, f\"{crop_name} Train\")\n    val_dataset = filter_valid_images(val_dataset, f\"{crop_name} Val\")\n    test_dataset = filter_valid_images(test_dataset, f\"{crop_name} Test\")\n\n    class_counts = Counter([train_dataset.dataset.targets[i] for i in train_dataset.indices])\n    print(f\"Class distribution for {crop_name}:\")\n    for i, class_name in enumerate(train_dataset.dataset.classes):\n        print(f\"  {class_name}: {class_counts[i]} samples\")\n\n    num_classes = len(train_dataset.dataset.classes)\n    print(f\"Loaded {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test images for {crop_name}\")\n    \n    return train_dataset, val_dataset, test_dataset, num_classes, train_dataset.dataset.classes\n\nclass OffsetLabelDataset:\n    def __init__(self, dataset, offset):\n        self.dataset = dataset\n        self.offset = offset\n\n    def __getitem__(self, idx):\n        img, label = self.dataset[idx]\n        return img, label + self.offset\n\n    def __len__(self):\n        return len(self.dataset)\n\ndef get_weighted_sampler(combined_dataset, crops, class_offsets):\n    # Fixed: Extract targets properly from combined dataset\n    targets = []\n    dataset_idx = 0\n    \n    for ds in combined_dataset.datasets:\n        crop_name = crops[dataset_idx]\n        offset = class_offsets[crop_name]\n        \n        # Get the actual targets from the subset\n        for idx in range(len(ds)):\n            _, label = ds[idx]  # This will give us the offset label\n            targets.append(label)\n        \n        dataset_idx += 1\n    \n    class_counts = Counter(targets)\n    num_classes = len(class_counts)\n    \n    # Calculate class weights\n    class_weights = {}\n    for class_idx, count in class_counts.items():\n        if count > 0:\n            class_weights[class_idx] = len(targets) / (num_classes * count)\n    \n    # Create sample weights\n    sample_weights = [class_weights[target] for target in targets]\n    \n    return WeightedRandomSampler(weights=sample_weights, num_samples=len(targets), replacement=True)\n\ndef train_model_improved(model, train_loader, val_loader, model_name, num_epochs=40, lr=1e-4, device=device):\n    model.to(device)\n    \n    # Fixed: Get targets from combined dataset properly\n    val_targets = []\n    for batch_inputs, batch_targets in val_loader:\n        val_targets.extend(batch_targets.numpy())\n    \n    class_weights = compute_class_weight('balanced', classes=np.unique(val_targets), y=val_targets)\n    criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n    \n    if isinstance(model, ImprovedCNNViTHybrid):\n        lr = 1e-5\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n    \n    best_acc = 0.0\n    best_loss = float('inf')\n    patience = 25\n    patience_counter = 0\n    best_model_state = None\n    \n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n    \n    for epoch in tqdm(range(num_epochs), desc=f\"{model_name} Training\"):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            if batch_idx % 50 == 0:\n                print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n\n        train_loss = running_loss / len(train_loader)\n        train_acc = correct / total\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                val_total += labels.size(0)\n                val_correct += predicted.eq(labels).sum().item()  # Fixed: += instead of =\n\n        val_loss /= len(val_loader)\n        val_acc = val_correct / val_total\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'{model_name} Epoch {epoch+1}/{num_epochs}: Train Loss {train_loss:.4f}, Acc {train_acc:.4f} | '\n              f'Val Loss {val_loss:.4f}, Acc {val_acc:.4f}')\n\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_loss = val_loss\n            best_model_state = model.state_dict()\n            torch.save({\n                'model_state_dict': best_model_state,\n                'optimizer_state_dict': optimizer.state_dict(),\n                'epoch': epoch,\n                'best_acc': best_acc,\n            }, os.path.join(MODEL_SAVE_PATH, f'{model_name}_best_model.pth'))\n            patience_counter = 0\n        elif val_loss > best_loss:\n            patience_counter += 1\n        else:\n            patience_counter = 0\n\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n        scheduler.step(val_loss)\n\n    # Load best model state\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n\n    return model, best_acc\n\ndef test_on_test_set(model, test_loader, model_name, class_names, device=device):\n    model.eval()\n    correct = 0\n    total = 0\n    class_correct = torch.zeros(len(class_names)).to(device)\n    class_total = torch.zeros(len(class_names)).to(device)\n    y_true = []\n    y_pred = []\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            for i in range(len(labels)):\n                label = labels[i]\n                class_correct[label] += (predicted[i] == label).item()\n                class_total[label] += 1\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(predicted.cpu().numpy())\n\n    acc = correct / total\n    print(f\"Test accuracy for {model_name}: {acc:.4f}\")\n    print(\"Per-class accuracy:\")\n    for i, (correct_count, total_count) in enumerate(zip(class_correct, class_total)):\n        if total_count > 0 and i < len(class_names):\n            print(f\"  {class_names[i]}: {correct_count/total_count:.4f} ({correct_count}/{total_count})\")\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, target_names=class_names[:max(y_true)+1], digits=4))\n    return acc, class_correct, class_total\n\ndef main_training():\n    crops = ['pepper bell', 'Wheat', 'Rubber', 'Coffee', 'Banana', 'Cardamom', 'Coconut', 'Mango', 'Potato', 'Rice']\n    batch_size = 16\n    model_name = 'all_crops_hybrid'\n\n    print(f\"\\n{'='*80}\")\n    print(\"LOADING COMBINED DATASETS FOR ALL CROPS\")\n    print('='*80)\n\n    train_datasets = []\n    val_datasets = []\n    test_datasets = []\n    global_classes = []\n    total_num_classes = 0\n    class_offsets = {}\n    offset = 0\n\n    total_train_samples = 0\n    total_val_samples = 0\n    total_test_samples = 0\n\n    for crop in crops:\n        train_ds, val_ds, test_ds, num_cls, cls_names = load_datasets_improved(crop)\n        full_names = [crop + '_' + name for name in cls_names]\n        global_classes.extend(full_names)\n        class_offsets[crop] = offset\n        train_datasets.append(OffsetLabelDataset(train_ds, offset))\n        val_datasets.append(OffsetLabelDataset(val_ds, offset))\n        test_datasets.append(OffsetLabelDataset(test_ds, offset))\n        offset += num_cls\n        total_num_classes += num_cls\n        total_train_samples += len(train_ds)\n        total_val_samples += len(val_ds)\n        total_test_samples += len(test_ds)\n\n    combined_train_dataset = ConcatDataset(train_datasets)\n    combined_val_dataset = ConcatDataset(val_datasets)\n    combined_test_dataset = ConcatDataset(test_datasets)\n\n    print(f\"\\nCombined Dataset Summary:\")\n    print(f\"  Training samples: {total_train_samples}\")\n    print(f\"  Validation samples: {total_val_samples}\")\n    print(f\"  Test samples: {total_test_samples}\")\n    print(f\"  Number of classes: {total_num_classes}\")\n    print(f\"  Classes: {global_classes}\")\n\n    print(f\"\\nAnalyzing class distribution and creating balanced sampler...\")\n    train_sampler = get_weighted_sampler(combined_train_dataset, crops, class_offsets)\n    \n    train_loader = DataLoader(combined_train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n    val_loader = DataLoader(combined_val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(combined_test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    print(f\"\\n{'-'*80}\")\n    print(\"TRAINING CNN-ViT HYBRID MODEL\")\n    print('-'*80)\n    hybrid_model = ImprovedCNNViTHybrid(num_classes=total_num_classes)\n    trained_model, best_acc = train_model_improved(\n        hybrid_model, train_loader, val_loader, model_name, num_epochs=40, device=device\n    )\n\n    print(f\"\\n{'-'*80}\")\n    print(\"TESTING ON TEST SET\")\n    print('-'*80)\n    test_acc, class_correct, class_total = test_on_test_set(trained_model, test_loader, model_name, global_classes, device)\n    \n    torch.save({\n        'model_state_dict': trained_model.state_dict(),\n        'model_type': 'hybrid',\n        'num_classes': total_num_classes,\n        'best_val_acc': best_acc,\n        'test_acc': test_acc,\n        'class_names': global_classes\n    }, os.path.join(MODEL_SAVE_PATH, f'{model_name}_final_model.pth'))\n\n    with open(os.path.join(MODEL_SAVE_PATH, f'{model_name}_classes.txt'), 'w') as f:\n        f.write('\\n'.join(global_classes))\n    \n    print(f\"\\n✅ Completed training: hybrid model with val_acc={best_acc:.4f}, test_acc={test_acc:.4f}\")\n\nif __name__ == \"__main__\":\n    main_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T22:54:50.064717Z","iopub.execute_input":"2025-09-10T22:54:50.065001Z","iopub.status.idle":"2025-09-11T03:09:08.653482Z","shell.execute_reply.started":"2025-09-10T22:54:50.064982Z","shell.execute_reply":"2025-09-11T03:09:08.652610Z"}},"outputs":[{"name":"stdout","text":"GPU Available: Tesla T4\nGPU Memory Available: 14.74 GB\n\n================================================================================\nLOADING COMBINED DATASETS FOR ALL CROPS\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Filtering valid images for pepper bell Train: 100%|██████████| 1753/1753 [00:14<00:00, 122.01it/s]\nFiltering valid images for pepper bell Val: 100%|██████████| 217/217 [00:00<00:00, 374.99it/s]\nFiltering valid images for pepper bell Test: 100%|██████████| 505/505 [00:01<00:00, 375.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Class distribution for pepper bell:\n  Bacterial spot: 704 samples\n  Healthy: 1049 samples\nLoaded 1753 train, 217 val, 505 test images for pepper bell\n","output_type":"stream"},{"name":"stderr","text":"Filtering valid images for Wheat Train: 100%|██████████| 3509/3509 [01:23<00:00, 41.91it/s]\nFiltering valid images for Wheat Val: 100%|██████████| 1000/1000 [00:17<00:00, 58.54it/s]\nFiltering valid images for Wheat Test: 100%|██████████| 500/500 [00:08<00:00, 59.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Class distribution for Wheat:\n  BlackPoint: 700 samples\n  FusariumFootRot: 709 samples\n  HealthyLeaf: 700 samples\n  LeafBlight: 700 samples\n  WheatBlast: 700 samples\nLoaded 3509 train, 1000 val, 500 test images for Wheat\n","output_type":"stream"},{"name":"stderr","text":"Filtering valid images for Rubber Train: 100%|██████████| 1212/1212 [00:14<00:00, 86.46it/s]\nFiltering valid images for Rubber Val: 100%|██████████| 182/182 [00:00<00:00, 201.96it/s]\nFiltering valid images for Rubber Test: 100%|██████████| 348/348 [00:01<00:00, 198.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Class distribution for Rubber:\n  Anthracnose: 210 samples\n  Dry leaf: 310 samples\n  Healthy: 354 samples\n  Leaf Spot: 338 samples\nLoaded 1212 train, 182 val, 348 test images for Rubber\n","output_type":"stream"},{"name":"stderr","text":"Filtering valid images for Coffee Train: 100%|██████████| 2215/2215 [00:17<00:00, 125.73it/s]\nFiltering valid images for Coffee Val: 100%|██████████| 337/337 [00:00<00:00, 415.53it/s]\nFiltering valid images for Coffee Test: 100%|██████████| 667/667 [00:01<00:00, 413.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Class distribution for Coffee:\n  Healthy: 717 samples\n  Phoma: 777 samples\n  leaf rust: 721 samples\nLoaded 2215 train, 337 val, 667 test images for Coffee\n","output_type":"stream"},{"name":"stderr","text":"Filtering valid images for Banana Train: 100%|██████████| 1120/1120 [00:09<00:00, 114.22it/s]\nFiltering valid images for Banana Val: 100%|██████████| 160/160 [00:00<00:00, 698.61it/s]\nFiltering valid images for Banana Test: 100%|██████████| 320/320 [00:00<00:00, 675.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Class distribution for Banana:\n  Cordana: 280 samples\n  Healthy: 280 samples\n  Sigatoka: 280 samples\n  pastalotiopsis: 280 samples\nLoaded 1120 train, 160 val, 320 test images for Banana\n","output_type":"stream"},{"name":"stderr","text":"Filtering valid images for Cardamom Train: 100%|██████████| 1560/1560 [00:27<00:00, 56.56it/s]\nFiltering valid images for Cardamom Val: 100%|██████████| 422/422 [00:04<00:00, 90.14it/s]\nFiltering valid images for Cardamom Test: 100%|██████████| 174/174 [00:02<00:00, 85.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Class distribution for Cardamom:\n  Blight: 225 samples\n  Healthy: 726 samples\n  Phylosticta: 609 samples\nLoaded 1560 train, 422 val, 174 test images for Cardamom\n","output_type":"stream"},{"name":"stderr","text":"Filtering valid images for Coconut Train: 100%|██████████| 3626/3626 [01:18<00:00, 46.25it/s]\nFiltering valid images for Coconut Val: 100%|██████████| 512/512 [00:09<00:00, 51.54it/s] \nFiltering valid images for Coconut Test: 100%|██████████| 910/910 [00:13<00:00, 69.45it/s] \n","output_type":"stream"},{"name":"stdout","text":"Class distribution for Coconut:\n  Caterpillar: 683 samples\n  Drying of leaves: 767 samples\n  Flaccidity: 758 samples\n  Healthy leaves: 86 samples\n  Yellowing: 748 samples\n  leaflet: 584 samples\nLoaded 3626 train, 512 val, 910 test images for Coconut\n","output_type":"stream"},{"name":"stderr","text":"Filtering valid images for Mango Train: 100%|██████████| 587/587 [00:05<00:00, 102.03it/s]\nFiltering valid images for Mango Val: 100%|██████████| 79/79 [00:00<00:00, 328.10it/s]\nFiltering valid images for Mango Test: 100%|██████████| 172/172 [00:00<00:00, 320.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Class distribution for Mango:\n  Alternaria: 112 samples\n  Anthracnose: 92 samples\n  Black Mould rot: 128 samples\n  Healthy: 144 samples\n  Stem and root: 111 samples\nLoaded 587 train, 79 val, 172 test images for Mango\n","output_type":"stream"},{"name":"stderr","text":"Filtering valid images for Potato Train: 100%|██████████| 900/900 [00:07<00:00, 119.72it/s]\nFiltering valid images for Potato Val: 100%|██████████| 300/300 [00:00<00:00, 406.69it/s]\nFiltering valid images for Potato Test: 100%|██████████| 300/300 [00:00<00:00, 405.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Class distribution for Potato:\n  Potato___Early_blight: 300 samples\n  Potato___Late_blight: 300 samples\n  Potato___healthy: 300 samples\nLoaded 900 train, 300 val, 300 test images for Potato\n","output_type":"stream"},{"name":"stderr","text":"Filtering valid images for Rice Train: 100%|██████████| 5507/5507 [01:40<00:00, 54.74it/s]\nFiltering valid images for Rice Val: 100%|██████████| 901/901 [00:10<00:00, 82.57it/s]\nFiltering valid images for Rice Test: 100%|██████████| 1849/1849 [00:21<00:00, 84.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Class distribution for Rice:\n  Bacterial leaf Blight: 507 samples\n  Healthy leaf: 335 samples\n  Rice: 510 samples\n  Rice Blast: 1708 samples\n  Tungro: 2447 samples\nLoaded 5507 train, 901 val, 1849 test images for Rice\n\nCombined Dataset Summary:\n  Training samples: 21989\n  Validation samples: 4110\n  Test samples: 5745\n  Number of classes: 40\n  Classes: ['pepper bell_Bacterial spot', 'pepper bell_Healthy', 'Wheat_BlackPoint', 'Wheat_FusariumFootRot', 'Wheat_HealthyLeaf', 'Wheat_LeafBlight', 'Wheat_WheatBlast', 'Rubber_Anthracnose', 'Rubber_Dry leaf', 'Rubber_Healthy', 'Rubber_Leaf Spot', 'Coffee_Healthy', 'Coffee_Phoma', 'Coffee_leaf rust', 'Banana_Cordana', 'Banana_Healthy', 'Banana_Sigatoka', 'Banana_pastalotiopsis', 'Cardamom_Blight', 'Cardamom_Healthy', 'Cardamom_Phylosticta', 'Coconut_Caterpillar', 'Coconut_Drying of leaves', 'Coconut_Flaccidity', 'Coconut_Healthy leaves', 'Coconut_Yellowing', 'Coconut_leaflet', 'Mango_Alternaria', 'Mango_Anthracnose', 'Mango_Black Mould rot', 'Mango_Healthy', 'Mango_Stem and root', 'Potato_Potato___Early_blight', 'Potato_Potato___Late_blight', 'Potato_Potato___healthy', 'Rice_Bacterial leaf Blight', 'Rice_Healthy leaf', 'Rice_Rice', 'Rice_Rice Blast', 'Rice_Tungro']\n\nAnalyzing class distribution and creating balanced sampler...\n\n--------------------------------------------------------------------------------\nTRAINING CNN-ViT HYBRID MODEL\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 215MB/s]\nall_crops_hybrid Training:   0%|          | 0/40 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 3.8492\nBatch 50/1375, Loss: 3.4421\nBatch 100/1375, Loss: 2.7897\nBatch 150/1375, Loss: 2.6589\nBatch 200/1375, Loss: 2.4672\nBatch 250/1375, Loss: 1.8202\nBatch 300/1375, Loss: 1.8765\nBatch 350/1375, Loss: 1.7661\nBatch 400/1375, Loss: 1.9433\nBatch 450/1375, Loss: 1.6691\nBatch 500/1375, Loss: 1.6887\nBatch 550/1375, Loss: 1.4189\nBatch 600/1375, Loss: 1.4576\nBatch 650/1375, Loss: 1.2173\nBatch 700/1375, Loss: 0.9041\nBatch 750/1375, Loss: 0.7848\nBatch 800/1375, Loss: 1.2116\nBatch 850/1375, Loss: 0.4467\nBatch 900/1375, Loss: 0.9779\nBatch 950/1375, Loss: 0.4368\nBatch 1000/1375, Loss: 0.3615\nBatch 1050/1375, Loss: 0.5777\nBatch 1100/1375, Loss: 0.5194\nBatch 1150/1375, Loss: 0.1946\nBatch 1200/1375, Loss: 0.9091\nBatch 1250/1375, Loss: 0.2791\nBatch 1300/1375, Loss: 0.3860\nBatch 1350/1375, Loss: 0.1906\nall_crops_hybrid Epoch 1/40: Train Loss 1.3277, Acc 0.5321 | Val Loss 0.8922, Acc 0.7844\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:   2%|▎         | 1/40 [05:45<3:44:22, 345.19s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.3535\nBatch 50/1375, Loss: 0.1994\nBatch 100/1375, Loss: 0.3243\nBatch 150/1375, Loss: 0.2927\nBatch 200/1375, Loss: 0.7947\nBatch 250/1375, Loss: 0.1325\nBatch 300/1375, Loss: 0.3038\nBatch 350/1375, Loss: 0.1305\nBatch 400/1375, Loss: 0.2346\nBatch 450/1375, Loss: 0.3361\nBatch 500/1375, Loss: 0.2688\nBatch 550/1375, Loss: 0.3272\nBatch 600/1375, Loss: 0.0577\nBatch 650/1375, Loss: 0.3647\nBatch 700/1375, Loss: 0.0886\nBatch 750/1375, Loss: 0.1797\nBatch 800/1375, Loss: 0.3195\nBatch 850/1375, Loss: 0.2044\nBatch 900/1375, Loss: 0.0165\nBatch 950/1375, Loss: 1.4319\nBatch 1000/1375, Loss: 0.0943\nBatch 1050/1375, Loss: 0.1793\nBatch 1100/1375, Loss: 0.5391\nBatch 1150/1375, Loss: 0.0387\nBatch 1200/1375, Loss: 0.1161\nBatch 1250/1375, Loss: 0.1117\nBatch 1300/1375, Loss: 0.1836\nBatch 1350/1375, Loss: 0.0930\nall_crops_hybrid Epoch 2/40: Train Loss 0.1896, Acc 0.9023 | Val Loss 0.3382, Acc 0.8701\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:   5%|▌         | 2/40 [11:45<3:44:11, 353.99s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.7462\nBatch 50/1375, Loss: 0.0292\nBatch 100/1375, Loss: 0.0600\nBatch 150/1375, Loss: 0.1556\nBatch 200/1375, Loss: 0.0322\nBatch 250/1375, Loss: 0.0297\nBatch 300/1375, Loss: 0.0181\nBatch 350/1375, Loss: 0.0613\nBatch 400/1375, Loss: 0.0551\nBatch 450/1375, Loss: 0.0163\nBatch 500/1375, Loss: 0.0060\nBatch 550/1375, Loss: 0.0944\nBatch 600/1375, Loss: 0.0158\nBatch 650/1375, Loss: 0.0093\nBatch 700/1375, Loss: 0.0220\nBatch 750/1375, Loss: 0.0242\nBatch 800/1375, Loss: 0.0274\nBatch 850/1375, Loss: 0.1518\nBatch 900/1375, Loss: 0.0106\nBatch 950/1375, Loss: 0.0665\nBatch 1000/1375, Loss: 0.0403\nBatch 1050/1375, Loss: 0.3464\nBatch 1100/1375, Loss: 0.0172\nBatch 1150/1375, Loss: 0.0598\nBatch 1200/1375, Loss: 0.0413\nBatch 1250/1375, Loss: 0.0848\nBatch 1300/1375, Loss: 0.0089\nBatch 1350/1375, Loss: 0.0219\nall_crops_hybrid Epoch 3/40: Train Loss 0.0818, Acc 0.9450 | Val Loss 0.2259, Acc 0.9085\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:   8%|▊         | 3/40 [17:44<3:39:45, 356.35s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.1366\nBatch 50/1375, Loss: 0.0048\nBatch 100/1375, Loss: 0.0047\nBatch 150/1375, Loss: 1.5036\nBatch 200/1375, Loss: 0.0170\nBatch 250/1375, Loss: 0.0489\nBatch 300/1375, Loss: 0.0130\nBatch 350/1375, Loss: 0.0087\nBatch 400/1375, Loss: 1.2143\nBatch 450/1375, Loss: 0.0776\nBatch 500/1375, Loss: 0.0590\nBatch 550/1375, Loss: 0.0610\nBatch 600/1375, Loss: 0.0168\nBatch 650/1375, Loss: 0.0794\nBatch 700/1375, Loss: 0.1086\nBatch 750/1375, Loss: 0.0069\nBatch 800/1375, Loss: 0.0294\nBatch 850/1375, Loss: 0.0774\nBatch 900/1375, Loss: 0.0036\nBatch 950/1375, Loss: 0.0590\nBatch 1000/1375, Loss: 0.0540\nBatch 1050/1375, Loss: 0.0279\nBatch 1100/1375, Loss: 0.0518\nBatch 1150/1375, Loss: 0.0336\nBatch 1200/1375, Loss: 0.0028\nBatch 1250/1375, Loss: 0.0045\nBatch 1300/1375, Loss: 0.0022\nBatch 1350/1375, Loss: 0.2593\nall_crops_hybrid Epoch 4/40: Train Loss 0.0518, Acc 0.9555 | Val Loss 0.2354, Acc 0.9124\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  10%|█         | 4/40 [23:44<3:34:40, 357.79s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0035\nBatch 50/1375, Loss: 0.0306\nBatch 100/1375, Loss: 0.0121\nBatch 150/1375, Loss: 0.0054\nBatch 200/1375, Loss: 0.0196\nBatch 250/1375, Loss: 0.4074\nBatch 300/1375, Loss: 0.0040\nBatch 350/1375, Loss: 0.0505\nBatch 400/1375, Loss: 0.0248\nBatch 450/1375, Loss: 0.1191\nBatch 500/1375, Loss: 0.1287\nBatch 550/1375, Loss: 0.0177\nBatch 600/1375, Loss: 0.0044\nBatch 650/1375, Loss: 0.0061\nBatch 700/1375, Loss: 0.0415\nBatch 750/1375, Loss: 0.0079\nBatch 800/1375, Loss: 0.0009\nBatch 850/1375, Loss: 0.1021\nBatch 900/1375, Loss: 0.0052\nBatch 950/1375, Loss: 0.3352\nBatch 1000/1375, Loss: 0.0010\nBatch 1050/1375, Loss: 0.0020\nBatch 1100/1375, Loss: 0.0052\nBatch 1150/1375, Loss: 0.0061\nBatch 1200/1375, Loss: 0.0094\nBatch 1250/1375, Loss: 0.0211\nBatch 1300/1375, Loss: 0.2460\nBatch 1350/1375, Loss: 0.0103\nall_crops_hybrid Epoch 5/40: Train Loss 0.0497, Acc 0.9633 | Val Loss 0.2069, Acc 0.9358\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  12%|█▎        | 5/40 [29:44<3:29:12, 358.63s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0018\nBatch 50/1375, Loss: 0.0683\nBatch 100/1375, Loss: 0.0589\nBatch 150/1375, Loss: 0.0140\nBatch 200/1375, Loss: 0.0014\nBatch 250/1375, Loss: 0.0027\nBatch 300/1375, Loss: 0.0021\nBatch 350/1375, Loss: 0.0912\nBatch 400/1375, Loss: 0.0379\nBatch 450/1375, Loss: 0.0214\nBatch 500/1375, Loss: 0.0216\nBatch 550/1375, Loss: 0.1958\nBatch 600/1375, Loss: 0.2188\nBatch 650/1375, Loss: 0.0006\nBatch 700/1375, Loss: 0.0032\nBatch 750/1375, Loss: 0.0015\nBatch 800/1375, Loss: 0.0054\nBatch 850/1375, Loss: 0.0007\nBatch 900/1375, Loss: 0.0036\nBatch 950/1375, Loss: 0.0040\nBatch 1000/1375, Loss: 0.0316\nBatch 1050/1375, Loss: 0.0043\nBatch 1100/1375, Loss: 0.0014\nBatch 1150/1375, Loss: 0.0987\nBatch 1200/1375, Loss: 0.0005\nBatch 1250/1375, Loss: 0.0221\nBatch 1300/1375, Loss: 0.0042\nBatch 1350/1375, Loss: 0.0009\nall_crops_hybrid Epoch 6/40: Train Loss 0.0350, Acc 0.9713 | Val Loss 0.1854, Acc 0.9465\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  15%|█▌        | 6/40 [35:44<3:23:30, 359.14s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0397\nBatch 50/1375, Loss: 0.0010\nBatch 100/1375, Loss: 0.0168\nBatch 150/1375, Loss: 1.2476\nBatch 200/1375, Loss: 0.0087\nBatch 250/1375, Loss: 0.0103\nBatch 300/1375, Loss: 0.0021\nBatch 350/1375, Loss: 0.0013\nBatch 400/1375, Loss: 0.0881\nBatch 450/1375, Loss: 0.0117\nBatch 500/1375, Loss: 0.0005\nBatch 550/1375, Loss: 0.0005\nBatch 600/1375, Loss: 0.0604\nBatch 650/1375, Loss: 0.0065\nBatch 700/1375, Loss: 0.0403\nBatch 750/1375, Loss: 0.0009\nBatch 800/1375, Loss: 0.0002\nBatch 850/1375, Loss: 0.0032\nBatch 900/1375, Loss: 0.0365\nBatch 950/1375, Loss: 0.0032\nBatch 1000/1375, Loss: 0.1021\nBatch 1050/1375, Loss: 0.0016\nBatch 1100/1375, Loss: 0.0251\nBatch 1150/1375, Loss: 0.0571\nBatch 1200/1375, Loss: 0.0169\nBatch 1250/1375, Loss: 0.0124\nBatch 1300/1375, Loss: 0.1074\nBatch 1350/1375, Loss: 0.0024\nall_crops_hybrid Epoch 7/40: Train Loss 0.0405, Acc 0.9743 | Val Loss 0.1964, Acc 0.9547\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  18%|█▊        | 7/40 [41:43<3:17:22, 358.87s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0301\nBatch 50/1375, Loss: 0.0007\nBatch 100/1375, Loss: 0.0008\nBatch 150/1375, Loss: 0.0103\nBatch 200/1375, Loss: 0.0174\nBatch 250/1375, Loss: 0.0005\nBatch 300/1375, Loss: 0.0114\nBatch 350/1375, Loss: 0.0010\nBatch 400/1375, Loss: 0.0387\nBatch 450/1375, Loss: 0.0009\nBatch 500/1375, Loss: 0.0026\nBatch 550/1375, Loss: 0.1041\nBatch 600/1375, Loss: 0.0006\nBatch 650/1375, Loss: 0.1266\nBatch 700/1375, Loss: 0.0041\nBatch 750/1375, Loss: 0.0041\nBatch 800/1375, Loss: 0.0007\nBatch 850/1375, Loss: 0.0159\nBatch 900/1375, Loss: 0.0027\nBatch 950/1375, Loss: 0.0010\nBatch 1000/1375, Loss: 0.0003\nBatch 1050/1375, Loss: 0.0474\nBatch 1100/1375, Loss: 0.0505\nBatch 1150/1375, Loss: 0.0011\nBatch 1200/1375, Loss: 0.0016\nBatch 1250/1375, Loss: 0.0016\nBatch 1300/1375, Loss: 0.0474\nBatch 1350/1375, Loss: 0.0020\nall_crops_hybrid Epoch 8/40: Train Loss 0.0377, Acc 0.9777 | Val Loss 0.1856, Acc 0.9579\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  20%|██        | 8/40 [47:43<3:11:37, 359.29s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0005\nBatch 50/1375, Loss: 0.2858\nBatch 100/1375, Loss: 0.0255\nBatch 150/1375, Loss: 0.0066\nBatch 200/1375, Loss: 0.0075\nBatch 250/1375, Loss: 0.0232\nBatch 300/1375, Loss: 0.0030\nBatch 350/1375, Loss: 0.0064\nBatch 400/1375, Loss: 0.0106\nBatch 450/1375, Loss: 0.0009\nBatch 500/1375, Loss: 0.0038\nBatch 550/1375, Loss: 0.0092\nBatch 600/1375, Loss: 0.0349\nBatch 650/1375, Loss: 0.0610\nBatch 700/1375, Loss: 0.0123\nBatch 750/1375, Loss: 0.0017\nBatch 800/1375, Loss: 0.0049\nBatch 850/1375, Loss: 0.0005\nBatch 900/1375, Loss: 0.0016\nBatch 950/1375, Loss: 0.1511\nBatch 1000/1375, Loss: 0.0002\nBatch 1050/1375, Loss: 0.0058\nBatch 1100/1375, Loss: 0.0014\nBatch 1150/1375, Loss: 0.0032\nBatch 1200/1375, Loss: 0.0002\nBatch 1250/1375, Loss: 0.0031\nBatch 1300/1375, Loss: 0.0256\nBatch 1350/1375, Loss: 0.0113\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  22%|██▎       | 9/40 [53:42<3:05:37, 359.29s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 9/40: Train Loss 0.0306, Acc 0.9794 | Val Loss 0.1909, Acc 0.9569\nBatch 0/1375, Loss: 0.0031\nBatch 50/1375, Loss: 0.0039\nBatch 100/1375, Loss: 0.0002\nBatch 150/1375, Loss: 0.0005\nBatch 200/1375, Loss: 0.0004\nBatch 250/1375, Loss: 0.0007\nBatch 300/1375, Loss: 0.0006\nBatch 350/1375, Loss: 0.1204\nBatch 400/1375, Loss: 0.0004\nBatch 450/1375, Loss: 0.0005\nBatch 500/1375, Loss: 0.0011\nBatch 550/1375, Loss: 0.0091\nBatch 600/1375, Loss: 0.0023\nBatch 650/1375, Loss: 0.0069\nBatch 700/1375, Loss: 0.0026\nBatch 750/1375, Loss: 0.0213\nBatch 800/1375, Loss: 0.0023\nBatch 850/1375, Loss: 0.0002\nBatch 900/1375, Loss: 0.0162\nBatch 950/1375, Loss: 0.3830\nBatch 1000/1375, Loss: 0.0161\nBatch 1050/1375, Loss: 0.0080\nBatch 1100/1375, Loss: 0.0032\nBatch 1150/1375, Loss: 0.0003\nBatch 1200/1375, Loss: 0.0031\nBatch 1250/1375, Loss: 0.0031\nBatch 1300/1375, Loss: 0.0399\nBatch 1350/1375, Loss: 0.1900\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  25%|██▌       | 10/40 [59:39<2:59:19, 358.64s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 10/40: Train Loss 0.0256, Acc 0.9828 | Val Loss 0.2201, Acc 0.9518\nBatch 0/1375, Loss: 0.4463\nBatch 50/1375, Loss: 0.0029\nBatch 100/1375, Loss: 0.0019\nBatch 150/1375, Loss: 0.0312\nBatch 200/1375, Loss: 0.0043\nBatch 250/1375, Loss: 0.0007\nBatch 300/1375, Loss: 0.0002\nBatch 350/1375, Loss: 0.4084\nBatch 400/1375, Loss: 0.2716\nBatch 450/1375, Loss: 0.0004\nBatch 500/1375, Loss: 0.0431\nBatch 550/1375, Loss: 0.0001\nBatch 600/1375, Loss: 0.0890\nBatch 650/1375, Loss: 0.0754\nBatch 700/1375, Loss: 0.0022\nBatch 750/1375, Loss: 0.0466\nBatch 800/1375, Loss: 0.0001\nBatch 850/1375, Loss: 0.0654\nBatch 900/1375, Loss: 0.0003\nBatch 950/1375, Loss: 0.0002\nBatch 1000/1375, Loss: 0.0003\nBatch 1050/1375, Loss: 0.0001\nBatch 1100/1375, Loss: 0.0042\nBatch 1150/1375, Loss: 0.0031\nBatch 1200/1375, Loss: 0.0003\nBatch 1250/1375, Loss: 0.0003\nBatch 1300/1375, Loss: 0.0001\nBatch 1350/1375, Loss: 0.0004\nall_crops_hybrid Epoch 11/40: Train Loss 0.0295, Acc 0.9831 | Val Loss 0.1768, Acc 0.9645\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  28%|██▊       | 11/40 [1:05:39<2:53:31, 359.01s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0002\nBatch 50/1375, Loss: 0.0033\nBatch 100/1375, Loss: 0.0006\nBatch 150/1375, Loss: 0.0003\nBatch 200/1375, Loss: 0.0079\nBatch 250/1375, Loss: 0.0597\nBatch 300/1375, Loss: 0.0003\nBatch 350/1375, Loss: 0.0021\nBatch 400/1375, Loss: 0.0004\nBatch 450/1375, Loss: 0.0297\nBatch 500/1375, Loss: 0.0022\nBatch 550/1375, Loss: 0.0001\nBatch 600/1375, Loss: 0.0057\nBatch 650/1375, Loss: 0.1074\nBatch 700/1375, Loss: 0.0005\nBatch 750/1375, Loss: 0.0044\nBatch 800/1375, Loss: 0.0042\nBatch 850/1375, Loss: 0.0014\nBatch 900/1375, Loss: 0.0004\nBatch 950/1375, Loss: 0.0478\nBatch 1000/1375, Loss: 0.0007\nBatch 1050/1375, Loss: 0.0292\nBatch 1100/1375, Loss: 0.0032\nBatch 1150/1375, Loss: 0.0001\nBatch 1200/1375, Loss: 0.0003\nBatch 1250/1375, Loss: 0.0002\nBatch 1300/1375, Loss: 0.0002\nBatch 1350/1375, Loss: 0.0524\nall_crops_hybrid Epoch 12/40: Train Loss 0.0317, Acc 0.9838 | Val Loss 0.1254, Acc 0.9759\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  30%|███       | 12/40 [1:11:39<2:47:40, 359.29s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0092\nBatch 50/1375, Loss: 0.0038\nBatch 100/1375, Loss: 0.0015\nBatch 150/1375, Loss: 0.0013\nBatch 200/1375, Loss: 0.0001\nBatch 250/1375, Loss: 0.0003\nBatch 300/1375, Loss: 0.0020\nBatch 350/1375, Loss: 0.0016\nBatch 400/1375, Loss: 0.0103\nBatch 450/1375, Loss: 0.0008\nBatch 500/1375, Loss: 0.0003\nBatch 550/1375, Loss: 0.0010\nBatch 600/1375, Loss: 0.0050\nBatch 650/1375, Loss: 0.1001\nBatch 700/1375, Loss: 0.0030\nBatch 750/1375, Loss: 0.0076\nBatch 800/1375, Loss: 0.0004\nBatch 850/1375, Loss: 0.0308\nBatch 900/1375, Loss: 0.0007\nBatch 950/1375, Loss: 0.0003\nBatch 1000/1375, Loss: 0.7952\nBatch 1050/1375, Loss: 0.0004\nBatch 1100/1375, Loss: 0.0015\nBatch 1150/1375, Loss: 0.0002\nBatch 1200/1375, Loss: 0.0001\nBatch 1250/1375, Loss: 0.0003\nBatch 1300/1375, Loss: 0.0216\nBatch 1350/1375, Loss: 0.0002\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  32%|███▎      | 13/40 [1:17:38<2:41:37, 359.15s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 13/40: Train Loss 0.0290, Acc 0.9855 | Val Loss 0.1423, Acc 0.9754\nBatch 0/1375, Loss: 0.0002\nBatch 50/1375, Loss: 0.0017\nBatch 100/1375, Loss: 0.0022\nBatch 150/1375, Loss: 0.0041\nBatch 200/1375, Loss: 0.0289\nBatch 250/1375, Loss: 0.0082\nBatch 300/1375, Loss: 0.0021\nBatch 350/1375, Loss: 0.0019\nBatch 400/1375, Loss: 0.0002\nBatch 450/1375, Loss: 0.0034\nBatch 500/1375, Loss: 0.0055\nBatch 550/1375, Loss: 0.0005\nBatch 600/1375, Loss: 0.0006\nBatch 650/1375, Loss: 0.0127\nBatch 700/1375, Loss: 0.0016\nBatch 750/1375, Loss: 0.0004\nBatch 800/1375, Loss: 0.0002\nBatch 850/1375, Loss: 0.0002\nBatch 900/1375, Loss: 0.0002\nBatch 950/1375, Loss: 0.0001\nBatch 1000/1375, Loss: 0.0032\nBatch 1050/1375, Loss: 0.0002\nBatch 1100/1375, Loss: 0.0002\nBatch 1150/1375, Loss: 0.0015\nBatch 1200/1375, Loss: 0.0067\nBatch 1250/1375, Loss: 0.0329\nBatch 1300/1375, Loss: 0.0013\nBatch 1350/1375, Loss: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  35%|███▌      | 14/40 [1:23:37<2:35:38, 359.19s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 14/40: Train Loss 0.0195, Acc 0.9874 | Val Loss 0.1599, Acc 0.9730\nBatch 0/1375, Loss: 0.0001\nBatch 50/1375, Loss: 0.0054\nBatch 100/1375, Loss: 0.1343\nBatch 150/1375, Loss: 0.0033\nBatch 200/1375, Loss: 0.0610\nBatch 250/1375, Loss: 0.0004\nBatch 300/1375, Loss: 0.0066\nBatch 350/1375, Loss: 2.4347\nBatch 400/1375, Loss: 0.0001\nBatch 450/1375, Loss: 0.0027\nBatch 500/1375, Loss: 0.0015\nBatch 550/1375, Loss: 0.0001\nBatch 600/1375, Loss: 0.0002\nBatch 650/1375, Loss: 0.0001\nBatch 700/1375, Loss: 0.0374\nBatch 750/1375, Loss: 0.0010\nBatch 800/1375, Loss: 0.0012\nBatch 850/1375, Loss: 0.0001\nBatch 900/1375, Loss: 0.0391\nBatch 950/1375, Loss: 0.0225\nBatch 1000/1375, Loss: 0.0021\nBatch 1050/1375, Loss: 0.0001\nBatch 1100/1375, Loss: 0.0395\nBatch 1150/1375, Loss: 0.0115\nBatch 1200/1375, Loss: 0.0011\nBatch 1250/1375, Loss: 0.0003\nBatch 1300/1375, Loss: 0.0096\nBatch 1350/1375, Loss: 0.0023\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  38%|███▊      | 15/40 [1:29:36<2:29:35, 359.03s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 15/40: Train Loss 0.0278, Acc 0.9869 | Val Loss 0.1441, Acc 0.9735\nBatch 0/1375, Loss: 0.0019\nBatch 50/1375, Loss: 0.0023\nBatch 100/1375, Loss: 1.9335\nBatch 150/1375, Loss: 0.0038\nBatch 200/1375, Loss: 0.0001\nBatch 250/1375, Loss: 0.0019\nBatch 300/1375, Loss: 0.0001\nBatch 350/1375, Loss: 0.0002\nBatch 400/1375, Loss: 0.0022\nBatch 450/1375, Loss: 0.0251\nBatch 500/1375, Loss: 0.0006\nBatch 550/1375, Loss: 0.0010\nBatch 600/1375, Loss: 0.0481\nBatch 650/1375, Loss: 0.0004\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.0001\nBatch 800/1375, Loss: 0.0594\nBatch 850/1375, Loss: 0.0044\nBatch 900/1375, Loss: 0.0048\nBatch 950/1375, Loss: 0.0001\nBatch 1000/1375, Loss: 0.0001\nBatch 1050/1375, Loss: 0.0001\nBatch 1100/1375, Loss: 0.0006\nBatch 1150/1375, Loss: 0.0680\nBatch 1200/1375, Loss: 0.0008\nBatch 1250/1375, Loss: 0.0019\nBatch 1300/1375, Loss: 0.0005\nBatch 1350/1375, Loss: 0.0030\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  40%|████      | 16/40 [1:35:36<2:23:45, 359.38s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 16/40: Train Loss 0.0207, Acc 0.9885 | Val Loss 0.1752, Acc 0.9710\nBatch 0/1375, Loss: 0.0144\nBatch 50/1375, Loss: 0.1904\nBatch 100/1375, Loss: 0.0012\nBatch 150/1375, Loss: 0.0003\nBatch 200/1375, Loss: 0.0001\nBatch 250/1375, Loss: 0.0023\nBatch 300/1375, Loss: 0.1096\nBatch 350/1375, Loss: 0.0001\nBatch 400/1375, Loss: 0.0021\nBatch 450/1375, Loss: 0.0001\nBatch 500/1375, Loss: 0.0087\nBatch 550/1375, Loss: 0.0002\nBatch 600/1375, Loss: 0.0001\nBatch 650/1375, Loss: 0.0053\nBatch 700/1375, Loss: 0.0002\nBatch 750/1375, Loss: 0.0319\nBatch 800/1375, Loss: 0.0001\nBatch 850/1375, Loss: 0.0001\nBatch 900/1375, Loss: 0.0016\nBatch 950/1375, Loss: 0.0005\nBatch 1000/1375, Loss: 0.0002\nBatch 1050/1375, Loss: 0.0000\nBatch 1100/1375, Loss: 0.0005\nBatch 1150/1375, Loss: 0.2112\nBatch 1200/1375, Loss: 0.2192\nBatch 1250/1375, Loss: 0.0002\nBatch 1300/1375, Loss: 0.0001\nBatch 1350/1375, Loss: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  42%|████▎     | 17/40 [1:41:36<2:17:51, 359.63s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 17/40: Train Loss 0.0224, Acc 0.9884 | Val Loss 0.1411, Acc 0.9742\nBatch 0/1375, Loss: 0.0001\nBatch 50/1375, Loss: 0.0001\nBatch 100/1375, Loss: 0.0396\nBatch 150/1375, Loss: 0.0002\nBatch 200/1375, Loss: 0.0060\nBatch 250/1375, Loss: 0.0032\nBatch 300/1375, Loss: 0.0009\nBatch 350/1375, Loss: 0.0004\nBatch 400/1375, Loss: 0.0007\nBatch 450/1375, Loss: 0.0044\nBatch 500/1375, Loss: 0.0018\nBatch 550/1375, Loss: 0.0017\nBatch 600/1375, Loss: 0.0014\nBatch 650/1375, Loss: 0.0001\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.0001\nBatch 800/1375, Loss: 0.0345\nBatch 850/1375, Loss: 0.0001\nBatch 900/1375, Loss: 0.0003\nBatch 950/1375, Loss: 0.0002\nBatch 1000/1375, Loss: 0.0004\nBatch 1050/1375, Loss: 0.0055\nBatch 1100/1375, Loss: 0.0003\nBatch 1150/1375, Loss: 0.0020\nBatch 1200/1375, Loss: 0.0001\nBatch 1250/1375, Loss: 0.0002\nBatch 1300/1375, Loss: 0.0000\nBatch 1350/1375, Loss: 0.0001\nall_crops_hybrid Epoch 18/40: Train Loss 0.0210, Acc 0.9889 | Val Loss 0.1241, Acc 0.9774\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  45%|████▌     | 18/40 [1:47:36<2:11:55, 359.78s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0121\nBatch 50/1375, Loss: 0.0067\nBatch 100/1375, Loss: 0.0001\nBatch 150/1375, Loss: 0.0004\nBatch 200/1375, Loss: 0.0212\nBatch 250/1375, Loss: 0.0170\nBatch 300/1375, Loss: 0.0004\nBatch 350/1375, Loss: 0.0389\nBatch 400/1375, Loss: 0.0001\nBatch 450/1375, Loss: 0.0004\nBatch 500/1375, Loss: 0.0001\nBatch 550/1375, Loss: 0.0029\nBatch 600/1375, Loss: 0.0205\nBatch 650/1375, Loss: 0.0000\nBatch 700/1375, Loss: 0.0033\nBatch 750/1375, Loss: 0.0001\nBatch 800/1375, Loss: 0.1148\nBatch 850/1375, Loss: 0.0017\nBatch 900/1375, Loss: 0.0007\nBatch 950/1375, Loss: 0.0014\nBatch 1000/1375, Loss: 0.0053\nBatch 1050/1375, Loss: 0.0001\nBatch 1100/1375, Loss: 0.2888\nBatch 1150/1375, Loss: 0.0001\nBatch 1200/1375, Loss: 0.0037\nBatch 1250/1375, Loss: 0.0002\nBatch 1300/1375, Loss: 0.0013\nBatch 1350/1375, Loss: 0.2093\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  48%|████▊     | 19/40 [1:53:36<2:05:54, 359.73s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 19/40: Train Loss 0.0200, Acc 0.9897 | Val Loss 0.1622, Acc 0.9708\nBatch 0/1375, Loss: 0.0273\nBatch 50/1375, Loss: 0.0022\nBatch 100/1375, Loss: 0.0000\nBatch 150/1375, Loss: 0.0450\nBatch 200/1375, Loss: 0.0001\nBatch 250/1375, Loss: 0.0001\nBatch 300/1375, Loss: 0.0009\nBatch 350/1375, Loss: 0.0010\nBatch 400/1375, Loss: 0.0008\nBatch 450/1375, Loss: 0.0069\nBatch 500/1375, Loss: 0.0008\nBatch 550/1375, Loss: 0.0013\nBatch 600/1375, Loss: 0.0318\nBatch 650/1375, Loss: 0.0001\nBatch 700/1375, Loss: 0.0036\nBatch 750/1375, Loss: 0.0002\nBatch 800/1375, Loss: 0.1736\nBatch 850/1375, Loss: 0.0005\nBatch 900/1375, Loss: 0.0014\nBatch 950/1375, Loss: 0.0048\nBatch 1000/1375, Loss: 0.0006\nBatch 1050/1375, Loss: 0.0001\nBatch 1100/1375, Loss: 0.0182\nBatch 1150/1375, Loss: 0.0002\nBatch 1200/1375, Loss: 0.0030\nBatch 1250/1375, Loss: 0.0307\nBatch 1300/1375, Loss: 0.0004\nBatch 1350/1375, Loss: 0.0011\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  50%|█████     | 20/40 [1:59:35<1:59:51, 359.59s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 20/40: Train Loss 0.0207, Acc 0.9892 | Val Loss 0.1463, Acc 0.9771\nBatch 0/1375, Loss: 0.0002\nBatch 50/1375, Loss: 0.1378\nBatch 100/1375, Loss: 0.0016\nBatch 150/1375, Loss: 0.0001\nBatch 200/1375, Loss: 0.0008\nBatch 250/1375, Loss: 0.0054\nBatch 300/1375, Loss: 0.0029\nBatch 350/1375, Loss: 0.0009\nBatch 400/1375, Loss: 0.0885\nBatch 450/1375, Loss: 0.0066\nBatch 500/1375, Loss: 0.0529\nBatch 550/1375, Loss: 0.0001\nBatch 600/1375, Loss: 0.0035\nBatch 650/1375, Loss: 0.0011\nBatch 700/1375, Loss: 0.0008\nBatch 750/1375, Loss: 0.0003\nBatch 800/1375, Loss: 0.0001\nBatch 850/1375, Loss: 0.0485\nBatch 900/1375, Loss: 0.0001\nBatch 950/1375, Loss: 0.0072\nBatch 1000/1375, Loss: 0.0531\nBatch 1050/1375, Loss: 0.0000\nBatch 1100/1375, Loss: 0.0001\nBatch 1150/1375, Loss: 0.0001\nBatch 1200/1375, Loss: 0.0001\nBatch 1250/1375, Loss: 0.0015\nBatch 1300/1375, Loss: 0.0048\nBatch 1350/1375, Loss: 0.0033\nall_crops_hybrid Epoch 21/40: Train Loss 0.0232, Acc 0.9885 | Val Loss 0.1260, Acc 0.9786\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  52%|█████▎    | 21/40 [2:05:34<1:53:50, 359.48s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0251\nBatch 50/1375, Loss: 0.0000\nBatch 100/1375, Loss: 0.0001\nBatch 150/1375, Loss: 0.0017\nBatch 200/1375, Loss: 0.0000\nBatch 250/1375, Loss: 0.0020\nBatch 300/1375, Loss: 0.0002\nBatch 350/1375, Loss: 0.0019\nBatch 400/1375, Loss: 0.0046\nBatch 450/1375, Loss: 0.0030\nBatch 500/1375, Loss: 0.0003\nBatch 550/1375, Loss: 0.0002\nBatch 600/1375, Loss: 0.0009\nBatch 650/1375, Loss: 0.0000\nBatch 700/1375, Loss: 0.0006\nBatch 750/1375, Loss: 0.0000\nBatch 800/1375, Loss: 0.0000\nBatch 850/1375, Loss: 0.0544\nBatch 900/1375, Loss: 0.0193\nBatch 950/1375, Loss: 0.0828\nBatch 1000/1375, Loss: 0.0273\nBatch 1050/1375, Loss: 0.0008\nBatch 1100/1375, Loss: 0.0136\nBatch 1150/1375, Loss: 0.0007\nBatch 1200/1375, Loss: 0.6040\nBatch 1250/1375, Loss: 0.0304\nBatch 1300/1375, Loss: 0.0001\nBatch 1350/1375, Loss: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  55%|█████▌    | 22/40 [2:11:33<1:47:44, 359.15s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 22/40: Train Loss 0.0167, Acc 0.9906 | Val Loss 0.1415, Acc 0.9757\nBatch 0/1375, Loss: 0.0000\nBatch 50/1375, Loss: 0.0013\nBatch 100/1375, Loss: 0.0008\nBatch 150/1375, Loss: 0.0002\nBatch 200/1375, Loss: 0.0019\nBatch 250/1375, Loss: 0.0001\nBatch 300/1375, Loss: 0.0001\nBatch 350/1375, Loss: 0.0069\nBatch 400/1375, Loss: 0.0007\nBatch 450/1375, Loss: 0.0007\nBatch 500/1375, Loss: 0.0002\nBatch 550/1375, Loss: 0.0000\nBatch 600/1375, Loss: 0.0013\nBatch 650/1375, Loss: 0.0115\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.0001\nBatch 800/1375, Loss: 0.0004\nBatch 850/1375, Loss: 0.0001\nBatch 900/1375, Loss: 0.0379\nBatch 950/1375, Loss: 0.0001\nBatch 1000/1375, Loss: 0.0002\nBatch 1050/1375, Loss: 0.0387\nBatch 1100/1375, Loss: 0.0001\nBatch 1150/1375, Loss: 0.0014\nBatch 1200/1375, Loss: 0.0039\nBatch 1250/1375, Loss: 0.0384\nBatch 1300/1375, Loss: 0.0011\nBatch 1350/1375, Loss: 0.0016\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  57%|█████▊    | 23/40 [2:17:32<1:41:46, 359.20s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 23/40: Train Loss 0.0146, Acc 0.9911 | Val Loss 0.1427, Acc 0.9774\nBatch 0/1375, Loss: 0.0001\nBatch 50/1375, Loss: 0.0001\nBatch 100/1375, Loss: 0.0000\nBatch 150/1375, Loss: 0.0011\nBatch 200/1375, Loss: 0.0001\nBatch 250/1375, Loss: 0.0000\nBatch 300/1375, Loss: 0.0007\nBatch 350/1375, Loss: 0.0003\nBatch 400/1375, Loss: 0.0000\nBatch 450/1375, Loss: 0.0001\nBatch 500/1375, Loss: 0.0096\nBatch 550/1375, Loss: 0.0001\nBatch 600/1375, Loss: 0.0001\nBatch 650/1375, Loss: 0.0002\nBatch 700/1375, Loss: 0.0026\nBatch 750/1375, Loss: 0.0000\nBatch 800/1375, Loss: 0.0011\nBatch 850/1375, Loss: 0.0000\nBatch 900/1375, Loss: 0.0002\nBatch 950/1375, Loss: 0.0009\nBatch 1000/1375, Loss: 0.1064\nBatch 1050/1375, Loss: 0.0878\nBatch 1100/1375, Loss: 0.0000\nBatch 1150/1375, Loss: 0.0001\nBatch 1200/1375, Loss: 0.0046\nBatch 1250/1375, Loss: 0.0138\nBatch 1300/1375, Loss: 0.0009\nBatch 1350/1375, Loss: 1.0042\nall_crops_hybrid Epoch 24/40: Train Loss 0.0278, Acc 0.9903 | Val Loss 0.1286, Acc 0.9803\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  60%|██████    | 24/40 [2:23:34<1:36:01, 360.07s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0001\nBatch 50/1375, Loss: 0.0001\nBatch 100/1375, Loss: 0.0013\nBatch 150/1375, Loss: 0.0001\nBatch 200/1375, Loss: 0.0000\nBatch 250/1375, Loss: 0.0004\nBatch 300/1375, Loss: 0.0001\nBatch 350/1375, Loss: 0.0000\nBatch 400/1375, Loss: 0.0001\nBatch 450/1375, Loss: 0.0002\nBatch 500/1375, Loss: 0.0476\nBatch 550/1375, Loss: 0.0013\nBatch 600/1375, Loss: 0.0031\nBatch 650/1375, Loss: 0.0013\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.0007\nBatch 800/1375, Loss: 0.0000\nBatch 850/1375, Loss: 0.0078\nBatch 900/1375, Loss: 0.0000\nBatch 950/1375, Loss: 0.0000\nBatch 1000/1375, Loss: 0.0001\nBatch 1050/1375, Loss: 0.0048\nBatch 1100/1375, Loss: 0.0012\nBatch 1150/1375, Loss: 0.0000\nBatch 1200/1375, Loss: 0.0009\nBatch 1250/1375, Loss: 0.0003\nBatch 1300/1375, Loss: 0.0000\nBatch 1350/1375, Loss: 0.0008\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  62%|██████▎   | 25/40 [2:29:33<1:29:53, 359.57s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 25/40: Train Loss 0.0107, Acc 0.9934 | Val Loss 0.1388, Acc 0.9796\nBatch 0/1375, Loss: 0.0000\nBatch 50/1375, Loss: 0.0000\nBatch 100/1375, Loss: 0.0000\nBatch 150/1375, Loss: 0.0002\nBatch 200/1375, Loss: 0.0002\nBatch 250/1375, Loss: 0.0000\nBatch 300/1375, Loss: 0.0007\nBatch 350/1375, Loss: 0.0012\nBatch 400/1375, Loss: 0.0011\nBatch 450/1375, Loss: 0.0001\nBatch 500/1375, Loss: 0.0001\nBatch 550/1375, Loss: 0.0001\nBatch 600/1375, Loss: 0.0000\nBatch 650/1375, Loss: 0.0013\nBatch 700/1375, Loss: 0.0002\nBatch 750/1375, Loss: 0.0028\nBatch 800/1375, Loss: 0.0001\nBatch 850/1375, Loss: 0.0259\nBatch 900/1375, Loss: 0.0247\nBatch 950/1375, Loss: 0.0071\nBatch 1000/1375, Loss: 0.0005\nBatch 1050/1375, Loss: 0.0089\nBatch 1100/1375, Loss: 0.0005\nBatch 1150/1375, Loss: 0.0502\nBatch 1200/1375, Loss: 0.0001\nBatch 1250/1375, Loss: 0.0008\nBatch 1300/1375, Loss: 0.0040\nBatch 1350/1375, Loss: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  65%|██████▌   | 26/40 [2:35:33<1:23:55, 359.68s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 26/40: Train Loss 0.0121, Acc 0.9937 | Val Loss 0.1379, Acc 0.9796\nBatch 0/1375, Loss: 0.0050\nBatch 50/1375, Loss: 0.0003\nBatch 100/1375, Loss: 0.0003\nBatch 150/1375, Loss: 0.0016\nBatch 200/1375, Loss: 0.0010\nBatch 250/1375, Loss: 0.0013\nBatch 300/1375, Loss: 0.0001\nBatch 350/1375, Loss: 0.0001\nBatch 400/1375, Loss: 0.0003\nBatch 450/1375, Loss: 0.0030\nBatch 500/1375, Loss: 0.0218\nBatch 550/1375, Loss: 0.0002\nBatch 600/1375, Loss: 0.0005\nBatch 650/1375, Loss: 0.0000\nBatch 700/1375, Loss: 0.0004\nBatch 750/1375, Loss: 0.0000\nBatch 800/1375, Loss: 0.0007\nBatch 850/1375, Loss: 0.0003\nBatch 900/1375, Loss: 0.0002\nBatch 950/1375, Loss: 0.0002\nBatch 1000/1375, Loss: 0.0001\nBatch 1050/1375, Loss: 0.0079\nBatch 1100/1375, Loss: 0.0000\nBatch 1150/1375, Loss: 0.0000\nBatch 1200/1375, Loss: 0.0001\nBatch 1250/1375, Loss: 0.0069\nBatch 1300/1375, Loss: 0.0048\nBatch 1350/1375, Loss: 0.0014\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  68%|██████▊   | 27/40 [2:41:32<1:17:56, 359.72s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 27/40: Train Loss 0.0131, Acc 0.9930 | Val Loss 0.1369, Acc 0.9781\nBatch 0/1375, Loss: 0.0027\nBatch 50/1375, Loss: 0.0000\nBatch 100/1375, Loss: 0.0000\nBatch 150/1375, Loss: 0.0004\nBatch 200/1375, Loss: 0.0013\nBatch 250/1375, Loss: 0.0020\nBatch 300/1375, Loss: 0.0000\nBatch 350/1375, Loss: 0.0000\nBatch 400/1375, Loss: 0.0001\nBatch 450/1375, Loss: 0.0000\nBatch 500/1375, Loss: 0.0000\nBatch 550/1375, Loss: 0.0012\nBatch 600/1375, Loss: 0.0023\nBatch 650/1375, Loss: 0.0025\nBatch 700/1375, Loss: 0.0011\nBatch 750/1375, Loss: 0.0000\nBatch 800/1375, Loss: 0.0000\nBatch 850/1375, Loss: 0.0047\nBatch 900/1375, Loss: 0.0000\nBatch 950/1375, Loss: 0.0000\nBatch 1000/1375, Loss: 0.0004\nBatch 1050/1375, Loss: 0.0001\nBatch 1100/1375, Loss: 0.0002\nBatch 1150/1375, Loss: 0.0543\nBatch 1200/1375, Loss: 0.0050\nBatch 1250/1375, Loss: 0.0013\nBatch 1300/1375, Loss: 0.0007\nBatch 1350/1375, Loss: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  70%|███████   | 28/40 [2:47:33<1:11:58, 359.87s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 28/40: Train Loss 0.0079, Acc 0.9946 | Val Loss 0.1431, Acc 0.9786\nBatch 0/1375, Loss: 0.0000\nBatch 50/1375, Loss: 0.0021\nBatch 100/1375, Loss: 0.0001\nBatch 150/1375, Loss: 0.0012\nBatch 200/1375, Loss: 0.0000\nBatch 250/1375, Loss: 0.0000\nBatch 300/1375, Loss: 0.0000\nBatch 350/1375, Loss: 0.0000\nBatch 400/1375, Loss: 0.0010\nBatch 450/1375, Loss: 0.0000\nBatch 500/1375, Loss: 0.0000\nBatch 550/1375, Loss: 0.0010\nBatch 600/1375, Loss: 0.0000\nBatch 650/1375, Loss: 0.0000\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.0001\nBatch 800/1375, Loss: 0.0009\nBatch 850/1375, Loss: 0.0015\nBatch 900/1375, Loss: 0.0000\nBatch 950/1375, Loss: 0.0000\nBatch 1000/1375, Loss: 0.0000\nBatch 1050/1375, Loss: 0.0000\nBatch 1100/1375, Loss: 0.0728\nBatch 1150/1375, Loss: 0.0004\nBatch 1200/1375, Loss: 0.0008\nBatch 1250/1375, Loss: 0.0000\nBatch 1300/1375, Loss: 0.0002\nBatch 1350/1375, Loss: 0.0021\nall_crops_hybrid Epoch 29/40: Train Loss 0.0086, Acc 0.9947 | Val Loss 0.1349, Acc 0.9805\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  72%|███████▎  | 29/40 [2:53:33<1:06:00, 360.01s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0400\nBatch 50/1375, Loss: 0.0023\nBatch 100/1375, Loss: 0.0002\nBatch 150/1375, Loss: 0.0011\nBatch 200/1375, Loss: 0.0001\nBatch 250/1375, Loss: 0.0000\nBatch 300/1375, Loss: 0.0001\nBatch 350/1375, Loss: 0.0016\nBatch 400/1375, Loss: 0.0001\nBatch 450/1375, Loss: 0.0001\nBatch 500/1375, Loss: 0.0009\nBatch 550/1375, Loss: 0.0000\nBatch 600/1375, Loss: 0.0000\nBatch 650/1375, Loss: 0.0001\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.0000\nBatch 800/1375, Loss: 0.0001\nBatch 850/1375, Loss: 0.0000\nBatch 900/1375, Loss: 0.0205\nBatch 950/1375, Loss: 0.0000\nBatch 1000/1375, Loss: 0.0000\nBatch 1050/1375, Loss: 0.0006\nBatch 1100/1375, Loss: 0.0000\nBatch 1150/1375, Loss: 0.0001\nBatch 1200/1375, Loss: 0.1769\nBatch 1250/1375, Loss: 0.0000\nBatch 1300/1375, Loss: 0.0007\nBatch 1350/1375, Loss: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  75%|███████▌  | 30/40 [2:59:33<59:58, 359.90s/it]  ","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 30/40: Train Loss 0.0124, Acc 0.9944 | Val Loss 0.1385, Acc 0.9791\nBatch 0/1375, Loss: 0.0000\nBatch 50/1375, Loss: 0.0001\nBatch 100/1375, Loss: 0.0001\nBatch 150/1375, Loss: 0.0001\nBatch 200/1375, Loss: 0.0000\nBatch 250/1375, Loss: 0.0046\nBatch 300/1375, Loss: 0.0003\nBatch 350/1375, Loss: 0.0005\nBatch 400/1375, Loss: 0.0676\nBatch 450/1375, Loss: 0.0008\nBatch 500/1375, Loss: 0.0000\nBatch 550/1375, Loss: 0.0000\nBatch 600/1375, Loss: 0.0007\nBatch 650/1375, Loss: 0.0002\nBatch 700/1375, Loss: 0.0001\nBatch 750/1375, Loss: 0.0012\nBatch 800/1375, Loss: 0.0000\nBatch 850/1375, Loss: 0.0000\nBatch 900/1375, Loss: 0.0010\nBatch 950/1375, Loss: 0.0000\nBatch 1000/1375, Loss: 0.0004\nBatch 1050/1375, Loss: 0.0004\nBatch 1100/1375, Loss: 0.0028\nBatch 1150/1375, Loss: 0.0014\nBatch 1200/1375, Loss: 0.0000\nBatch 1250/1375, Loss: 0.0666\nBatch 1300/1375, Loss: 0.0009\nBatch 1350/1375, Loss: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  78%|███████▊  | 31/40 [3:05:31<53:56, 359.59s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 31/40: Train Loss 0.0075, Acc 0.9940 | Val Loss 0.1363, Acc 0.9805\nBatch 0/1375, Loss: 0.0049\nBatch 50/1375, Loss: 0.0001\nBatch 100/1375, Loss: 0.0072\nBatch 150/1375, Loss: 0.0001\nBatch 200/1375, Loss: 0.0000\nBatch 250/1375, Loss: 0.0001\nBatch 300/1375, Loss: 0.0015\nBatch 350/1375, Loss: 0.0000\nBatch 400/1375, Loss: 0.0000\nBatch 450/1375, Loss: 0.0006\nBatch 500/1375, Loss: 0.0000\nBatch 550/1375, Loss: 0.0001\nBatch 600/1375, Loss: 0.0000\nBatch 650/1375, Loss: 0.0513\nBatch 700/1375, Loss: 0.0302\nBatch 750/1375, Loss: 0.0025\nBatch 800/1375, Loss: 0.0001\nBatch 850/1375, Loss: 0.0001\nBatch 900/1375, Loss: 0.0000\nBatch 950/1375, Loss: 0.0000\nBatch 1000/1375, Loss: 0.0028\nBatch 1050/1375, Loss: 0.0008\nBatch 1100/1375, Loss: 0.0001\nBatch 1150/1375, Loss: 0.0147\nBatch 1200/1375, Loss: 0.0001\nBatch 1250/1375, Loss: 0.0006\nBatch 1300/1375, Loss: 0.0000\nBatch 1350/1375, Loss: 0.0005\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  80%|████████  | 32/40 [3:11:30<47:54, 359.26s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 32/40: Train Loss 0.0063, Acc 0.9955 | Val Loss 0.1372, Acc 0.9798\nBatch 0/1375, Loss: 0.0017\nBatch 50/1375, Loss: 0.0000\nBatch 100/1375, Loss: 0.0001\nBatch 150/1375, Loss: 0.0001\nBatch 200/1375, Loss: 0.0000\nBatch 250/1375, Loss: 0.0078\nBatch 300/1375, Loss: 0.0000\nBatch 350/1375, Loss: 0.0508\nBatch 400/1375, Loss: 0.0000\nBatch 450/1375, Loss: 0.0000\nBatch 500/1375, Loss: 0.0000\nBatch 550/1375, Loss: 0.0477\nBatch 600/1375, Loss: 0.0000\nBatch 650/1375, Loss: 0.0001\nBatch 700/1375, Loss: 0.0006\nBatch 750/1375, Loss: 0.0540\nBatch 800/1375, Loss: 0.0025\nBatch 850/1375, Loss: 0.0333\nBatch 900/1375, Loss: 0.0020\nBatch 950/1375, Loss: 0.0000\nBatch 1000/1375, Loss: 0.0014\nBatch 1050/1375, Loss: 0.0060\nBatch 1100/1375, Loss: 0.0000\nBatch 1150/1375, Loss: 0.0000\nBatch 1200/1375, Loss: 0.0222\nBatch 1250/1375, Loss: 0.0001\nBatch 1300/1375, Loss: 0.0006\nBatch 1350/1375, Loss: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  82%|████████▎ | 33/40 [3:17:28<41:53, 359.01s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 33/40: Train Loss 0.0062, Acc 0.9955 | Val Loss 0.1434, Acc 0.9796\nBatch 0/1375, Loss: 0.0000\nBatch 50/1375, Loss: 0.0015\nBatch 100/1375, Loss: 0.0000\nBatch 150/1375, Loss: 0.0001\nBatch 200/1375, Loss: 0.0001\nBatch 250/1375, Loss: 0.0000\nBatch 300/1375, Loss: 0.0000\nBatch 350/1375, Loss: 0.0034\nBatch 400/1375, Loss: 0.0000\nBatch 450/1375, Loss: 0.0187\nBatch 500/1375, Loss: 0.0011\nBatch 550/1375, Loss: 0.0000\nBatch 600/1375, Loss: 0.0006\nBatch 650/1375, Loss: 0.0011\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.0024\nBatch 800/1375, Loss: 0.0001\nBatch 850/1375, Loss: 0.0000\nBatch 900/1375, Loss: 0.0001\nBatch 950/1375, Loss: 0.0005\nBatch 1000/1375, Loss: 0.0516\nBatch 1050/1375, Loss: 0.0000\nBatch 1100/1375, Loss: 0.0000\nBatch 1150/1375, Loss: 0.0000\nBatch 1200/1375, Loss: 0.0000\nBatch 1250/1375, Loss: 0.0000\nBatch 1300/1375, Loss: 0.0001\nBatch 1350/1375, Loss: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  85%|████████▌ | 34/40 [3:23:28<35:55, 359.29s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 34/40: Train Loss 0.0073, Acc 0.9947 | Val Loss 0.1453, Acc 0.9781\nBatch 0/1375, Loss: 0.0022\nBatch 50/1375, Loss: 0.0005\nBatch 100/1375, Loss: 0.0000\nBatch 150/1375, Loss: 1.1820\nBatch 200/1375, Loss: 0.0004\nBatch 250/1375, Loss: 0.0187\nBatch 300/1375, Loss: 0.0000\nBatch 350/1375, Loss: 0.0007\nBatch 400/1375, Loss: 0.0006\nBatch 450/1375, Loss: 0.0009\nBatch 500/1375, Loss: 0.0003\nBatch 550/1375, Loss: 0.0002\nBatch 600/1375, Loss: 0.0001\nBatch 650/1375, Loss: 0.0019\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.2553\nBatch 800/1375, Loss: 0.0000\nBatch 850/1375, Loss: 0.0005\nBatch 900/1375, Loss: 0.0431\nBatch 950/1375, Loss: 0.0008\nBatch 1000/1375, Loss: 0.0000\nBatch 1050/1375, Loss: 0.0000\nBatch 1100/1375, Loss: 0.0578\nBatch 1150/1375, Loss: 0.0000\nBatch 1200/1375, Loss: 0.8383\nBatch 1250/1375, Loss: 0.0000\nBatch 1300/1375, Loss: 0.0008\nBatch 1350/1375, Loss: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  88%|████████▊ | 35/40 [3:29:27<29:56, 359.28s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 35/40: Train Loss 0.0067, Acc 0.9960 | Val Loss 0.1472, Acc 0.9796\nBatch 0/1375, Loss: 0.0000\nBatch 50/1375, Loss: 0.0003\nBatch 100/1375, Loss: 0.0000\nBatch 150/1375, Loss: 0.0000\nBatch 200/1375, Loss: 0.0000\nBatch 250/1375, Loss: 0.0001\nBatch 300/1375, Loss: 0.0000\nBatch 350/1375, Loss: 0.0000\nBatch 400/1375, Loss: 0.0000\nBatch 450/1375, Loss: 0.0000\nBatch 500/1375, Loss: 0.0002\nBatch 550/1375, Loss: 0.0000\nBatch 600/1375, Loss: 0.0004\nBatch 650/1375, Loss: 0.0000\nBatch 700/1375, Loss: 0.0013\nBatch 750/1375, Loss: 0.3276\nBatch 800/1375, Loss: 0.0650\nBatch 850/1375, Loss: 0.0007\nBatch 900/1375, Loss: 0.0040\nBatch 950/1375, Loss: 0.0019\nBatch 1000/1375, Loss: 0.0000\nBatch 1050/1375, Loss: 0.0000\nBatch 1100/1375, Loss: 0.0000\nBatch 1150/1375, Loss: 0.0000\nBatch 1200/1375, Loss: 0.0621\nBatch 1250/1375, Loss: 0.0001\nBatch 1300/1375, Loss: 0.0000\nBatch 1350/1375, Loss: 0.0002\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  90%|█████████ | 36/40 [3:35:26<23:55, 358.94s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 36/40: Train Loss 0.0109, Acc 0.9951 | Val Loss 0.1383, Acc 0.9798\nBatch 0/1375, Loss: 0.0000\nBatch 50/1375, Loss: 0.0000\nBatch 100/1375, Loss: 0.0000\nBatch 150/1375, Loss: 0.0000\nBatch 200/1375, Loss: 0.0000\nBatch 250/1375, Loss: 0.0000\nBatch 300/1375, Loss: 0.0000\nBatch 350/1375, Loss: 0.0015\nBatch 400/1375, Loss: 0.0000\nBatch 450/1375, Loss: 0.0000\nBatch 500/1375, Loss: 0.0000\nBatch 550/1375, Loss: 0.0009\nBatch 600/1375, Loss: 0.0004\nBatch 650/1375, Loss: 0.0001\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.0000\nBatch 800/1375, Loss: 0.0000\nBatch 850/1375, Loss: 0.0000\nBatch 900/1375, Loss: 0.0000\nBatch 950/1375, Loss: 0.0000\nBatch 1000/1375, Loss: 0.0000\nBatch 1050/1375, Loss: 0.0004\nBatch 1100/1375, Loss: 0.0000\nBatch 1150/1375, Loss: 0.0001\nBatch 1200/1375, Loss: 0.0009\nBatch 1250/1375, Loss: 0.0013\nBatch 1300/1375, Loss: 0.0000\nBatch 1350/1375, Loss: 0.0001\nall_crops_hybrid Epoch 37/40: Train Loss 0.0055, Acc 0.9955 | Val Loss 0.1329, Acc 0.9808\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  92%|█████████▎| 37/40 [3:41:27<17:58, 359.62s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0579\nBatch 50/1375, Loss: 0.0338\nBatch 100/1375, Loss: 0.0000\nBatch 150/1375, Loss: 0.0000\nBatch 200/1375, Loss: 0.0000\nBatch 250/1375, Loss: 0.0001\nBatch 300/1375, Loss: 0.0022\nBatch 350/1375, Loss: 0.0001\nBatch 400/1375, Loss: 0.0009\nBatch 450/1375, Loss: 0.0010\nBatch 500/1375, Loss: 0.0000\nBatch 550/1375, Loss: 0.0000\nBatch 600/1375, Loss: 0.0000\nBatch 650/1375, Loss: 0.0002\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.1324\nBatch 800/1375, Loss: 0.0058\nBatch 850/1375, Loss: 0.0002\nBatch 900/1375, Loss: 0.0000\nBatch 950/1375, Loss: 0.0000\nBatch 1000/1375, Loss: 0.0000\nBatch 1050/1375, Loss: 0.0001\nBatch 1100/1375, Loss: 0.0001\nBatch 1150/1375, Loss: 0.0000\nBatch 1200/1375, Loss: 0.0006\nBatch 1250/1375, Loss: 0.0000\nBatch 1300/1375, Loss: 0.0000\nBatch 1350/1375, Loss: 0.0341\nall_crops_hybrid Epoch 38/40: Train Loss 0.0065, Acc 0.9956 | Val Loss 0.1318, Acc 0.9813\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  95%|█████████▌| 38/40 [3:47:27<11:59, 359.89s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0/1375, Loss: 0.0000\nBatch 50/1375, Loss: 0.0000\nBatch 100/1375, Loss: 0.0514\nBatch 150/1375, Loss: 0.0004\nBatch 200/1375, Loss: 0.0000\nBatch 250/1375, Loss: 0.0000\nBatch 300/1375, Loss: 0.0033\nBatch 350/1375, Loss: 0.0000\nBatch 400/1375, Loss: 0.0001\nBatch 450/1375, Loss: 0.0000\nBatch 500/1375, Loss: 0.0001\nBatch 550/1375, Loss: 0.0000\nBatch 600/1375, Loss: 0.0004\nBatch 650/1375, Loss: 0.0001\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.0003\nBatch 800/1375, Loss: 0.0000\nBatch 850/1375, Loss: 0.0000\nBatch 900/1375, Loss: 0.0000\nBatch 950/1375, Loss: 0.0013\nBatch 1000/1375, Loss: 0.0007\nBatch 1050/1375, Loss: 0.0000\nBatch 1100/1375, Loss: 0.0006\nBatch 1150/1375, Loss: 0.0000\nBatch 1200/1375, Loss: 0.0000\nBatch 1250/1375, Loss: 0.0000\nBatch 1300/1375, Loss: 0.0002\nBatch 1350/1375, Loss: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training:  98%|█████████▊| 39/40 [3:53:27<05:59, 359.78s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 39/40: Train Loss 0.0046, Acc 0.9964 | Val Loss 0.1391, Acc 0.9810\nBatch 0/1375, Loss: 0.0017\nBatch 50/1375, Loss: 0.0019\nBatch 100/1375, Loss: 0.0014\nBatch 150/1375, Loss: 0.0000\nBatch 200/1375, Loss: 0.0000\nBatch 250/1375, Loss: 0.0024\nBatch 300/1375, Loss: 0.0000\nBatch 350/1375, Loss: 0.0012\nBatch 400/1375, Loss: 0.0000\nBatch 450/1375, Loss: 0.0000\nBatch 500/1375, Loss: 0.0006\nBatch 550/1375, Loss: 0.0005\nBatch 600/1375, Loss: 0.0000\nBatch 650/1375, Loss: 0.0000\nBatch 700/1375, Loss: 0.0000\nBatch 750/1375, Loss: 0.0001\nBatch 800/1375, Loss: 0.0000\nBatch 850/1375, Loss: 0.0000\nBatch 900/1375, Loss: 0.0007\nBatch 950/1375, Loss: 0.0000\nBatch 1000/1375, Loss: 0.0000\nBatch 1050/1375, Loss: 0.0000\nBatch 1100/1375, Loss: 0.0000\nBatch 1150/1375, Loss: 0.0001\nBatch 1200/1375, Loss: 0.0005\nBatch 1250/1375, Loss: 0.0015\nBatch 1300/1375, Loss: 0.0000\nBatch 1350/1375, Loss: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"all_crops_hybrid Training: 100%|██████████| 40/40 [3:59:26<00:00, 359.15s/it]","output_type":"stream"},{"name":"stdout","text":"all_crops_hybrid Epoch 40/40: Train Loss 0.0050, Acc 0.9961 | Val Loss 0.1378, Acc 0.9813\n\n--------------------------------------------------------------------------------\nTESTING ON TEST SET\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test accuracy for all_crops_hybrid: 0.9535\nPer-class accuracy:\n  pepper bell_Bacterial spot: 1.0000 (205.0/205.0)\n  pepper bell_Healthy: 1.0000 (300.0/300.0)\n  Wheat_BlackPoint: 1.0000 (100.0/100.0)\n  Wheat_FusariumFootRot: 1.0000 (100.0/100.0)\n  Wheat_HealthyLeaf: 1.0000 (100.0/100.0)\n  Wheat_LeafBlight: 1.0000 (100.0/100.0)\n  Wheat_WheatBlast: 1.0000 (100.0/100.0)\n  Rubber_Anthracnose: 1.0000 (62.0/62.0)\n  Rubber_Dry leaf: 1.0000 (90.0/90.0)\n  Rubber_Healthy: 1.0000 (101.0/101.0)\n  Rubber_Leaf Spot: 1.0000 (95.0/95.0)\n  Coffee_Healthy: 0.9873 (234.0/237.0)\n  Coffee_Phoma: 0.6233 (139.0/223.0)\n  Coffee_leaf rust: 0.9420 (195.0/207.0)\n  Banana_Cordana: 1.0000 (80.0/80.0)\n  Banana_Healthy: 1.0000 (80.0/80.0)\n  Banana_Sigatoka: 0.8875 (71.0/80.0)\n  Banana_pastalotiopsis: 0.9500 (76.0/80.0)\n  Cardamom_Blight: 1.0000 (28.0/28.0)\n  Cardamom_Healthy: 1.0000 (79.0/79.0)\n  Cardamom_Phylosticta: 1.0000 (67.0/67.0)\n  Coconut_Caterpillar: 1.0000 (202.0/202.0)\n  Coconut_Drying of leaves: 1.0000 (94.0/94.0)\n  Coconut_Flaccidity: 1.0000 (218.0/218.0)\n  Coconut_Healthy leaves: 1.0000 (25.0/25.0)\n  Coconut_Yellowing: 0.9953 (211.0/212.0)\n  Coconut_leaflet: 1.0000 (159.0/159.0)\n  Mango_Alternaria: 0.8889 (32.0/36.0)\n  Mango_Anthracnose: 0.3462 (9.0/26.0)\n  Mango_Black Mould rot: 0.7027 (26.0/37.0)\n  Mango_Healthy: 1.0000 (42.0/42.0)\n  Mango_Stem and root: 0.8065 (25.0/31.0)\n  Potato_Potato___Early_blight: 1.0000 (100.0/100.0)\n  Potato_Potato___Late_blight: 0.9900 (99.0/100.0)\n  Potato_Potato___healthy: 1.0000 (100.0/100.0)\n  Rice_Bacterial leaf Blight: 0.8506 (131.0/154.0)\n  Rice_Healthy leaf: 1.0000 (101.0/101.0)\n  Rice_Rice: 1.0000 (166.0/166.0)\n  Rice_Rice Blast: 0.8808 (665.0/755.0)\n  Rice_Tungro: 0.9970 (671.0/673.0)\n\nClassification Report:\n                              precision    recall  f1-score   support\n\n  pepper bell_Bacterial spot     1.0000    1.0000    1.0000       205\n         pepper bell_Healthy     1.0000    1.0000    1.0000       300\n            Wheat_BlackPoint     1.0000    1.0000    1.0000       100\n       Wheat_FusariumFootRot     1.0000    1.0000    1.0000       100\n           Wheat_HealthyLeaf     1.0000    1.0000    1.0000       100\n            Wheat_LeafBlight     1.0000    1.0000    1.0000       100\n            Wheat_WheatBlast     1.0000    1.0000    1.0000       100\n          Rubber_Anthracnose     1.0000    1.0000    1.0000        62\n             Rubber_Dry leaf     1.0000    1.0000    1.0000        90\n              Rubber_Healthy     1.0000    1.0000    1.0000       101\n            Rubber_Leaf Spot     1.0000    1.0000    1.0000        95\n              Coffee_Healthy     0.9915    0.9873    0.9894       237\n                Coffee_Phoma     0.9267    0.6233    0.7453       223\n            Coffee_leaf rust     0.6940    0.9420    0.7992       207\n              Banana_Cordana     0.9091    1.0000    0.9524        80\n              Banana_Healthy     0.9524    1.0000    0.9756        80\n             Banana_Sigatoka     0.9861    0.8875    0.9342        80\n       Banana_pastalotiopsis     1.0000    0.9500    0.9744        80\n             Cardamom_Blight     1.0000    1.0000    1.0000        28\n            Cardamom_Healthy     1.0000    1.0000    1.0000        79\n        Cardamom_Phylosticta     1.0000    1.0000    1.0000        67\n         Coconut_Caterpillar     1.0000    1.0000    1.0000       202\n    Coconut_Drying of leaves     1.0000    1.0000    1.0000        94\n          Coconut_Flaccidity     0.9954    1.0000    0.9977       218\n      Coconut_Healthy leaves     1.0000    1.0000    1.0000        25\n           Coconut_Yellowing     1.0000    0.9953    0.9976       212\n             Coconut_leaflet     1.0000    1.0000    1.0000       159\n            Mango_Alternaria     0.6038    0.8889    0.7191        36\n           Mango_Anthracnose     0.7500    0.3462    0.4737        26\n       Mango_Black Mould rot     0.7027    0.7027    0.7027        37\n               Mango_Healthy     0.9545    1.0000    0.9767        42\n         Mango_Stem and root     0.9615    0.8065    0.8772        31\nPotato_Potato___Early_blight     0.9901    1.0000    0.9950       100\n Potato_Potato___Late_blight     1.0000    0.9900    0.9950       100\n     Potato_Potato___healthy     1.0000    1.0000    1.0000       100\n  Rice_Bacterial leaf Blight     0.5955    0.8506    0.7005       154\n           Rice_Healthy leaf     1.0000    1.0000    1.0000       101\n                   Rice_Rice     1.0000    1.0000    1.0000       166\n             Rice_Rice Blast     0.9666    0.8808    0.9217       755\n                 Rice_Tungro     0.9955    0.9970    0.9963       673\n\n                    accuracy                         0.9535      5745\n                   macro avg     0.9494    0.9462    0.9431      5745\n                weighted avg     0.9615    0.9535    0.9542      5745\n\n\n✅ Completed training: hybrid model with val_acc=0.9813, test_acc=0.9535\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"!ls -l /kaggle/working/AI_Farmer_Models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:28:28.852402Z","iopub.execute_input":"2025-09-11T03:28:28.852742Z","iopub.status.idle":"2025-09-11T03:28:29.012164Z","shell.execute_reply.started":"2025-09-11T03:28:28.852712Z","shell.execute_reply":"2025-09-11T03:28:29.011260Z"}},"outputs":[{"name":"stdout","text":"total 658380\n-rw-r--r-- 1 root root 505499806 Sep 11 02:56 all_crops_hybrid_best_model.pth\n-rw-r--r-- 1 root root       747 Sep 11 03:09 all_crops_hybrid_classes.txt\n-rw-r--r-- 1 root root 168673193 Sep 11 03:09 all_crops_hybrid_final_model.pth\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"ritam = '/kaggle/working/AI_Farmer_Models'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:29:48.008475Z","iopub.execute_input":"2025-09-11T03:29:48.009057Z","iopub.status.idle":"2025-09-11T03:29:48.012732Z","shell.execute_reply.started":"2025-09-11T03:29:48.009030Z","shell.execute_reply":"2025-09-11T03:29:48.011643Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"ritam","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:30:02.360098Z","iopub.execute_input":"2025-09-11T03:30:02.360838Z","iopub.status.idle":"2025-09-11T03:30:02.367114Z","shell.execute_reply.started":"2025-09-11T03:30:02.360814Z","shell.execute_reply":"2025-09-11T03:30:02.366418Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/AI_Farmer_Models'"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/working/AI_Farmer_Models/all_crops_hybrid_final_model.pth\")\nprint(checkpoint.keys())  # Should include 'model_state_dict', 'model_type', 'num_classes', 'best_val_acc', 'test_acc', 'class_names'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:30:27.567118Z","iopub.execute_input":"2025-09-11T03:30:27.567435Z","iopub.status.idle":"2025-09-11T03:30:27.761567Z","shell.execute_reply.started":"2025-09-11T03:30:27.567415Z","shell.execute_reply":"2025-09-11T03:30:27.760813Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['model_state_dict', 'model_type', 'num_classes', 'best_val_acc', 'test_acc', 'class_names'])\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"with open(\"/kaggle/working/AI_Farmer_Models/all_crops_hybrid_classes.txt\", 'r') as f:\n    class_names = f.read().splitlines()\nprint(class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:31:26.605663Z","iopub.execute_input":"2025-09-11T03:31:26.605950Z","iopub.status.idle":"2025-09-11T03:31:26.611023Z","shell.execute_reply.started":"2025-09-11T03:31:26.605929Z","shell.execute_reply":"2025-09-11T03:31:26.610190Z"}},"outputs":[{"name":"stdout","text":"['pepper bell_Bacterial spot', 'pepper bell_Healthy', 'Wheat_BlackPoint', 'Wheat_FusariumFootRot', 'Wheat_HealthyLeaf', 'Wheat_LeafBlight', 'Wheat_WheatBlast', 'Rubber_Anthracnose', 'Rubber_Dry leaf', 'Rubber_Healthy', 'Rubber_Leaf Spot', 'Coffee_Healthy', 'Coffee_Phoma', 'Coffee_leaf rust', 'Banana_Cordana', 'Banana_Healthy', 'Banana_Sigatoka', 'Banana_pastalotiopsis', 'Cardamom_Blight', 'Cardamom_Healthy', 'Cardamom_Phylosticta', 'Coconut_Caterpillar', 'Coconut_Drying of leaves', 'Coconut_Flaccidity', 'Coconut_Healthy leaves', 'Coconut_Yellowing', 'Coconut_leaflet', 'Mango_Alternaria', 'Mango_Anthracnose', 'Mango_Black Mould rot', 'Mango_Healthy', 'Mango_Stem and root', 'Potato_Potato___Early_blight', 'Potato_Potato___Late_blight', 'Potato_Potato___healthy', 'Rice_Bacterial leaf Blight', 'Rice_Healthy leaf', 'Rice_Rice', 'Rice_Rice Blast', 'Rice_Tungro']\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"### Testing via image url","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nfrom PIL import Image\nimport os\nimport requests\nfrom io import BytesIO\nimport base64\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the model architecture\nclass ImprovedCNNViTHybrid(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(ImprovedCNNViTHybrid, self).__init__()\n        self.backbone = models.resnet50(pretrained=pretrained)\n        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n        \n        self.feature_dim = 2048\n        self.patch_size = 7\n        self.num_patches = 49\n        self.embedding_dim = 768\n        \n        self.feature_projection = nn.Sequential(\n            nn.AdaptiveAvgPool2d((7, 7)),\n            nn.Conv2d(self.feature_dim, self.embedding_dim, kernel_size=1),\n            nn.BatchNorm2d(self.embedding_dim),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embedding_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, self.embedding_dim))\n        self.dropout = nn.Dropout(0.3)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.embedding_dim, nhead=8, dim_feedforward=2048, dropout=0.3, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=3)\n        \n        self.classifier = nn.Sequential(\n            nn.LayerNorm(self.embedding_dim),\n            nn.Dropout(0.3),\n            nn.Linear(self.embedding_dim, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(512, num_classes)\n        )\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        \n    def forward(self, x):\n        B = x.shape[0]\n        features = self.backbone(x)\n        features = self.feature_projection(features)\n        features = features.flatten(2).transpose(1, 2)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        features = torch.cat([cls_tokens, features], dim=1)\n        features = features + self.pos_embed\n        features = self.dropout(features)\n        encoded = self.transformer(features)\n        cls_output = encoded[:, 0]\n        return self.classifier(cls_output)\n\n# Load the trained model\ndef load_trained_model(model_path, device=device):\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file {model_path} not found.\")\n    checkpoint = torch.load(model_path, map_location=device)\n    num_classes = checkpoint.get('num_classes', 40)  # Default to 40 if not found\n    model = ImprovedCNNViTHybrid(num_classes=num_classes, pretrained=False)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(device)\n    model.eval()\n    \n    # Extract class names from checkpoint, fallback to file if not present\n    class_names = checkpoint.get('class_names', None)\n    if class_names is None:\n        class_names_file = model_path.replace('_final_model.pth', '_classes.txt')\n        if os.path.exists(class_names_file):\n            with open(class_names_file, 'r') as f:\n                class_names = f.read().splitlines()\n        else:\n            raise FileNotFoundError(f\"Class names file {class_names_file} not found.\")\n    return model, class_names\n\n# Define the same transformations used during training\ndef get_test_transform():\n    return transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.Grayscale(num_output_channels=3),  # Adjust based on your dataset (remove if RGB)\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n# Download image from URL and test the model (updated for base64)\ndef test_on_image_url(model, class_names, image_url, device=device, threshold=0.8):\n    # Handle base64 URL\n    if image_url.startswith('data:image'):\n        try:\n            header, encoded = image_url.split(',', 1)\n            img_data = base64.b64decode(encoded)\n            img = Image.open(BytesIO(img_data)).convert('RGB')\n        except Exception as e:\n            print(f\"Error decoding base64 image: {e}\")\n            return \"Invalid image. Contact local Kisan Officer at 1800-180-1551.\", None\n    else:\n        # Download from regular URL\n        try:\n            response = requests.get(image_url, timeout=10)\n            response.raise_for_status()  # Check for HTTP errors\n            img = Image.open(BytesIO(response.content)).convert('RGB')\n        except Exception as e:\n            print(f\"Error downloading or loading image from {image_url}: {e}\")\n            return \"Invalid image. Contact local Kisan Officer at 1800-180-1551.\", None\n\n    # Preprocess the image\n    transform = get_test_transform()\n    img = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n\n    # Make prediction\n    with torch.no_grad():\n        output = model(img)\n        prob = torch.softmax(output, dim=1)[0]\n        pred_idx = prob.argmax().item()\n        confidence = prob[pred_idx].item()\n\n    # Map prediction to class name\n    if pred_idx >= len(class_names):\n        print(f\"Warning: Predicted index {pred_idx} exceeds number of classes {len(class_names)}\")\n        return \"Invalid prediction. Contact local Kisan Officer at 1800-180-1551.\", None\n\n    predicted_class = class_names[pred_idx]\n\n    # Determine if it's a disease (assuming class names end with '_Healthy' or disease names)\n    is_disease = not predicted_class.endswith('_Healthy')\n    disease_status = \"Disease detected\" if is_disease else \"No disease detected (Healthy)\"\n\n    # Decision based on confidence\n    if confidence < threshold:\n        print(f\"Prediction for {image_url}: Not confident (Confidence: {confidence:.4f})\")\n        return f\"Not confident in prediction. {disease_status}. Contact local Kisan Officer at 1800-180-1551.\", confidence\n    else:\n        print(f\"Prediction for {image_url}: {predicted_class} (Confidence: {confidence:.4f})\")\n        return f\"{predicted_class} - {disease_status} (Confidence: {confidence:.4f})\", confidence\n\n# Main execution\nif __name__ == \"__main__\":\n    # Paths to the model and class names file\n    model_path = \"/kaggle/working/AI_Farmer_Models/all_crops_hybrid_final_model.pth\"\n    class_names_file = \"/kaggle/working/AI_Farmer_Models/all_crops_hybrid_classes.txt\"\n\n    # Load the model and class names\n    model, class_names = load_trained_model(model_path)\n    # Optionally override with file content if needed\n    with open(class_names_file, 'r') as f:\n        class_names = f.read().splitlines()\n\n    # Provide the URL of the image to test\n    image_url = \"https://apps.lucidcentral.org/pppw_v10/images/entities/mango_bacterial_black_spot_213/mangobactspot1.jpg\"\n\n    # Test the model on the image from URL\n    prediction, confidence = test_on_image_url(model, class_names, image_url)\n    print(f\"Final Prediction: {prediction}, Confidence: {confidence:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T04:13:06.065655Z","iopub.execute_input":"2025-09-11T04:13:06.065918Z","iopub.status.idle":"2025-09-11T04:13:06.953073Z","shell.execute_reply.started":"2025-09-11T04:13:06.065898Z","shell.execute_reply":"2025-09-11T04:13:06.952358Z"}},"outputs":[{"name":"stdout","text":"Prediction for https://apps.lucidcentral.org/pppw_v10/images/entities/mango_bacterial_black_spot_213/mangobactspot1.jpg: Rubber_Leaf Spot (Confidence: 0.8608)\nFinal Prediction: Rubber_Leaf Spot - Disease detected (Confidence: 0.8608), Confidence: 0.8608\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"### Testing via downloaded images","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nfrom PIL import Image\nimport os\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the model architecture\nclass ImprovedCNNViTHybrid(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(ImprovedCNNViTHybrid, self).__init__()\n        self.backbone = models.resnet50(pretrained=pretrained)\n        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n        \n        self.feature_dim = 2048\n        self.patch_size = 7\n        self.num_patches = 49\n        self.embedding_dim = 768\n        \n        self.feature_projection = nn.Sequential(\n            nn.AdaptiveAvgPool2d((7, 7)),\n            nn.Conv2d(self.feature_dim, self.embedding_dim, kernel_size=1),\n            nn.BatchNorm2d(self.embedding_dim),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embedding_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, self.embedding_dim))\n        self.dropout = nn.Dropout(0.3)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.embedding_dim, nhead=8, dim_feedforward=2048, dropout=0.3, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=3)\n        \n        self.classifier = nn.Sequential(\n            nn.LayerNorm(self.embedding_dim),\n            nn.Dropout(0.3),\n            nn.Linear(self.embedding_dim, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(512, num_classes)\n        )\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        \n    def forward(self, x):\n        B = x.shape[0]\n        features = self.backbone(x)\n        features = self.feature_projection(features)\n        features = features.flatten(2).transpose(1, 2)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        features = torch.cat([cls_tokens, features], dim=1)\n        features = features + self.pos_embed\n        features = self.dropout(features)\n        encoded = self.transformer(features)\n        cls_output = encoded[:, 0]\n        return self.classifier(cls_output)\n\n# Load the trained model\ndef load_trained_model(model_path, device=device):\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file {model_path} not found.\")\n    checkpoint = torch.load(model_path, map_location=device)\n    num_classes = checkpoint.get('num_classes', 40)  # Default to 40 if not found\n    model = ImprovedCNNViTHybrid(num_classes=num_classes, pretrained=False)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(device)\n    model.eval()\n    \n    # Extract class names from checkpoint, fallback to file if not present\n    class_names = checkpoint.get('class_names', None)\n    if class_names is None:\n        class_names_file = model_path.replace('_final_model.pth', '_classes.txt')\n        if os.path.exists(class_names_file):\n            with open(class_names_file, 'r') as f:\n                class_names = f.read().splitlines()\n        else:\n            raise FileNotFoundError(f\"Class names file {class_names_file} not found.\")\n    return model, class_names\n\n# Define the same transformations used during training\ndef get_test_transform():\n    return transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.Grayscale(num_output_channels=3),  # Adjust based on your dataset (remove if RGB)\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n# Test the model on an image from the input directory\ndef test_on_image(model, class_names, image_path, device=device, threshold=0.8):\n    # Load and preprocess the image\n    try:\n        img = Image.open(image_path).convert('RGB')\n        print(f\"Loaded image from {image_path}, size: {img.size}\")\n    except Exception as e:\n        print(f\"Error loading image {image_path}: {e}\")\n        return \"Invalid image. Contact local Kisan Officer at 1800-180-1551.\", None\n\n    transform = get_test_transform()\n    img = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n\n    # Make prediction\n    with torch.no_grad():\n        output = model(img)\n        prob = torch.softmax(output, dim=1)[0]\n        pred_idx = prob.argmax().item()\n        confidence = prob[pred_idx].item()\n        print(f\"Raw probabilities: {prob}\")\n        print(f\"Predicted index: {pred_idx}, Confidence: {confidence}\")\n\n    # Map prediction to class name\n    if pred_idx >= len(class_names):\n        print(f\"Warning: Predicted index {pred_idx} exceeds number of classes {len(class_names)}\")\n        return \"Invalid prediction. Contact local Kisan Officer at 1800-180-1551.\", None\n\n    predicted_class = class_names[pred_idx]\n\n    # Determine if it's a disease (assuming class names end with '_Healthy' or disease names)\n    is_disease = not predicted_class.endswith('_Healthy')\n    disease_status = \"Disease detected\" if is_disease else \"No disease detected (Healthy)\"\n\n    # Decision based on confidence\n    if confidence < threshold:\n        print(f\"Prediction for {image_path}: Not confident (Confidence: {confidence:.4f})\")\n        return f\"Not confident in prediction. {disease_status}. Contact local Kisan Officer at 1800-180-1551.\", confidence\n    else:\n        print(f\"Prediction for {image_path}: {predicted_class} (Confidence: {confidence:.4f})\")\n        return f\"{predicted_class} - {disease_status} (Confidence: {confidence:.4f})\", confidence\n\n# Main execution\nif __name__ == \"__main__\":\n    # Paths to the model and class names file\n    model_path = \"/kaggle/working/AI_Farmer_Models/all_crops_hybrid_final_model.pth\"\n    class_names_file = \"/kaggle/working/AI_Farmer_Models/all_crops_hybrid_classes.txt\"\n\n    # Load the model and class names\n    model, class_names = load_trained_model(model_path)\n    with open(class_names_file, 'r') as f:\n        class_names = f.read().splitlines()\n    print(f\"Loaded {len(class_names)} classes: {class_names[:5]}...\")\n\n    # Path to the uploaded image in the input directory\n    image_path = \"/kaggle/input/testimgs/Test image/1.jpeg\"  # Adjust this path based on your upload\n\n    # Test the model on the image\n    prediction, confidence = test_on_image(model, class_names, image_path)\n    if confidence is not None:\n        print(f\"Final Prediction: {prediction}, Confidence: {confidence:.4f}\")\n    else:\n        print(f\"Final Prediction: {prediction}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T14:20:28.919678Z","iopub.execute_input":"2025-09-11T14:20:28.919883Z","iopub.status.idle":"2025-09-11T14:20:40.732603Z","shell.execute_reply.started":"2025-09-11T14:20:28.919857Z","shell.execute_reply":"2025-09-11T14:20:40.731319Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2614630179.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# Load the model and class names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2614630179.py\u001b[0m in \u001b[0;36mload_trained_model\u001b[0;34m(model_path, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model file {model_path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_classes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Default to 40 if not found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Model file /kaggle/working/AI_Farmer_Models/all_crops_hybrid_final_model.pth not found."],"ename":"FileNotFoundError","evalue":"Model file /kaggle/working/AI_Farmer_Models/all_crops_hybrid_final_model.pth not found.","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}